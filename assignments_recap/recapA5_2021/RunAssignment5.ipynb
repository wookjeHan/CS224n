{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RunAssignment5.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMO3gbpRXWiqlZP3EAlsxSP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkD-gr4hi0Js","executionInfo":{"status":"ok","timestamp":1621389191427,"user_tz":-540,"elapsed":24047,"user":{"displayName":"우기온앤오프","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLw9XcTXYIvff_IdkIHpgDs6qse_Q2E6OOzd59Ig=s64","userId":"03343870549788815937"}},"outputId":"58d04f2b-2578-4ddc-fa9b-8d4f2f601d43"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'\n","# folders.\n","# e.g. 'cs231n/assignments/assignment1/cs231n/'\n","FOLDERNAME = 'cs224n/assignments/recapA5_2021'\n","\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd drive/MyDrive/cs224n/assignments/recapA5_2021/\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/cs224n/assignments/recapA5_2021\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dna_9lUslMq0","executionInfo":{"status":"ok","timestamp":1621348566519,"user_tz":-540,"elapsed":7167,"user":{"displayName":"우기온앤오프","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLw9XcTXYIvff_IdkIHpgDs6qse_Q2E6OOzd59Ig=s64","userId":"03343870549788815937"}},"outputId":"3e27dbd2-5c6d-4bca-a220-41b90dea8c51"},"source":["!python src/dataset.py namedata"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data has 418352 characters, 256 unique.\n","x: Where was Khatchig Mouradian born?⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Where was Jacob Henry Studer born?⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Where was John Stephen born?⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Where was Georgina Willis born?⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peRFA7wwvpHv","executionInfo":{"status":"ok","timestamp":1621349188496,"user_tz":-540,"elapsed":276800,"user":{"displayName":"우기온앤오프","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLw9XcTXYIvff_IdkIHpgDs6qse_Q2E6OOzd59Ig=s64","userId":"03343870549788815937"}},"outputId":"74123d23-cdcc-4b30-b9ed-bb85cd748141"},"source":["# Train on the names dataset\n","!python src/run.py finetune vanilla wiki.txt \\\n","--writing_params_path vanilla.model.params \\\n","--finetune_corpus_path birth_places_train.tsv\n","# Evaluate on the dev set, writing out predictions\n","!python src/run.py evaluate vanilla wiki.txt \\\n","--reading_params_path vanilla.model.params \\\n","--eval_corpus_path birth_dev.tsv \\\n","--outputs_path vanilla.nopretrain.dev.predictions\n","# Evaluate on the test set, writing out predictions\n","!python src/run.py evaluate vanilla wiki.txt \\\n","--reading_params_path vanilla.model.params \\\n","--eval_corpus_path birth_test_inputs.tsv \\\n","--outputs_path vanilla.nopretrain.test.predictions"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 7: train loss 3.48277. lr 5.999844e-04: 100% 8/8 [00:01<00:00,  4.54it/s]\n","epoch 2 iter 7: train loss 2.65207. lr 5.999351e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 3 iter 7: train loss 2.31483. lr 5.998521e-04: 100% 8/8 [00:01<00:00,  4.67it/s]\n","epoch 4 iter 7: train loss 2.15858. lr 5.997352e-04: 100% 8/8 [00:01<00:00,  4.68it/s]\n","epoch 5 iter 7: train loss 2.06153. lr 5.995847e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 6 iter 7: train loss 1.99426. lr 5.994004e-04: 100% 8/8 [00:01<00:00,  4.66it/s]\n","epoch 7 iter 7: train loss 1.94920. lr 5.991823e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 8 iter 7: train loss 1.90122. lr 5.989306e-04: 100% 8/8 [00:01<00:00,  4.66it/s]\n","epoch 9 iter 7: train loss 1.85145. lr 5.986453e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 10 iter 7: train loss 1.80278. lr 5.983263e-04: 100% 8/8 [00:01<00:00,  4.64it/s]\n","epoch 11 iter 7: train loss 1.73223. lr 5.979737e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 12 iter 7: train loss 1.68221. lr 5.975876e-04: 100% 8/8 [00:01<00:00,  4.59it/s]\n","epoch 13 iter 7: train loss 1.59504. lr 5.971680e-04: 100% 8/8 [00:01<00:00,  4.66it/s]\n","epoch 14 iter 7: train loss 1.53161. lr 5.967149e-04: 100% 8/8 [00:01<00:00,  4.59it/s]\n","epoch 15 iter 7: train loss 1.44925. lr 5.962284e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 16 iter 7: train loss 1.37931. lr 5.957086e-04: 100% 8/8 [00:01<00:00,  4.64it/s]\n","epoch 17 iter 7: train loss 1.32045. lr 5.951554e-04: 100% 8/8 [00:01<00:00,  4.60it/s]\n","epoch 18 iter 7: train loss 1.22846. lr 5.945690e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 19 iter 7: train loss 1.18497. lr 5.939495e-04: 100% 8/8 [00:01<00:00,  4.64it/s]\n","epoch 20 iter 7: train loss 1.12865. lr 5.932969e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 21 iter 7: train loss 1.06218. lr 5.926112e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 22 iter 7: train loss 1.03179. lr 5.918926e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 23 iter 7: train loss 0.98825. lr 5.911412e-04: 100% 8/8 [00:01<00:00,  4.59it/s]\n","epoch 24 iter 7: train loss 0.93479. lr 5.903569e-04: 100% 8/8 [00:01<00:00,  4.66it/s]\n","epoch 25 iter 7: train loss 0.86317. lr 5.895400e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 26 iter 7: train loss 0.84248. lr 5.886905e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 27 iter 7: train loss 0.82864. lr 5.878084e-04: 100% 8/8 [00:01<00:00,  4.58it/s]\n","epoch 28 iter 7: train loss 0.79375. lr 5.868940e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 29 iter 7: train loss 0.76559. lr 5.859473e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 30 iter 7: train loss 0.75749. lr 5.849683e-04: 100% 8/8 [00:01<00:00,  4.68it/s]\n","epoch 31 iter 7: train loss 0.72855. lr 5.839573e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 32 iter 7: train loss 0.71520. lr 5.829143e-04: 100% 8/8 [00:01<00:00,  4.64it/s]\n","epoch 33 iter 7: train loss 0.67160. lr 5.818395e-04: 100% 8/8 [00:01<00:00,  4.60it/s]\n","epoch 34 iter 7: train loss 0.67472. lr 5.807329e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 35 iter 7: train loss 0.64847. lr 5.795947e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 36 iter 7: train loss 0.63985. lr 5.784251e-04: 100% 8/8 [00:01<00:00,  4.60it/s]\n","epoch 37 iter 7: train loss 0.62945. lr 5.772241e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 38 iter 7: train loss 0.62447. lr 5.759918e-04: 100% 8/8 [00:01<00:00,  4.67it/s]\n","epoch 39 iter 7: train loss 0.59766. lr 5.747285e-04: 100% 8/8 [00:01<00:00,  4.59it/s]\n","epoch 40 iter 7: train loss 0.58954. lr 5.734343e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 41 iter 7: train loss 0.58468. lr 5.721093e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 42 iter 7: train loss 0.55125. lr 5.707537e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 43 iter 7: train loss 0.52902. lr 5.693675e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 44 iter 7: train loss 0.52795. lr 5.679511e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 45 iter 7: train loss 0.51689. lr 5.665044e-04: 100% 8/8 [00:01<00:00,  4.59it/s]\n","epoch 46 iter 7: train loss 0.53128. lr 5.650278e-04: 100% 8/8 [00:01<00:00,  4.69it/s]\n","epoch 47 iter 7: train loss 0.52776. lr 5.635213e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 48 iter 7: train loss 0.50500. lr 5.619852e-04: 100% 8/8 [00:01<00:00,  4.66it/s]\n","epoch 49 iter 7: train loss 0.49736. lr 5.604195e-04: 100% 8/8 [00:01<00:00,  4.64it/s]\n","epoch 50 iter 7: train loss 0.50219. lr 5.588246e-04: 100% 8/8 [00:01<00:00,  4.60it/s]\n","epoch 51 iter 7: train loss 0.46812. lr 5.572005e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 52 iter 7: train loss 0.44575. lr 5.555474e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 53 iter 7: train loss 0.43056. lr 5.538656e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 54 iter 7: train loss 0.43036. lr 5.521552e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 55 iter 7: train loss 0.41456. lr 5.504164e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 56 iter 7: train loss 0.40542. lr 5.486494e-04: 100% 8/8 [00:01<00:00,  4.64it/s]\n","epoch 57 iter 7: train loss 0.39906. lr 5.468544e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 58 iter 7: train loss 0.38698. lr 5.450316e-04: 100% 8/8 [00:01<00:00,  4.57it/s]\n","epoch 59 iter 7: train loss 0.38377. lr 5.431812e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 60 iter 7: train loss 0.38018. lr 5.413034e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 61 iter 7: train loss 0.36423. lr 5.393985e-04: 100% 8/8 [00:01<00:00,  4.58it/s]\n","epoch 62 iter 7: train loss 0.35035. lr 5.374666e-04: 100% 8/8 [00:01<00:00,  4.60it/s]\n","epoch 63 iter 7: train loss 0.33531. lr 5.355080e-04: 100% 8/8 [00:01<00:00,  4.71it/s]\n","epoch 64 iter 7: train loss 0.32093. lr 5.335229e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 65 iter 7: train loss 0.32827. lr 5.315115e-04: 100% 8/8 [00:01<00:00,  4.64it/s]\n","epoch 66 iter 7: train loss 0.28705. lr 5.294740e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 67 iter 7: train loss 0.29118. lr 5.274107e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 68 iter 7: train loss 0.27853. lr 5.253217e-04: 100% 8/8 [00:01<00:00,  4.63it/s]\n","epoch 69 iter 7: train loss 0.24735. lr 5.232074e-04: 100% 8/8 [00:01<00:00,  4.65it/s]\n","epoch 70 iter 7: train loss 0.25129. lr 5.210680e-04: 100% 8/8 [00:01<00:00,  4.61it/s]\n","epoch 71 iter 7: train loss 0.25634. lr 5.189037e-04: 100% 8/8 [00:01<00:00,  4.60it/s]\n","epoch 72 iter 7: train loss 0.22623. lr 5.167147e-04: 100% 8/8 [00:01<00:00,  4.59it/s]\n","epoch 73 iter 7: train loss 0.22474. lr 5.145014e-04: 100% 8/8 [00:01<00:00,  4.66it/s]\n","epoch 74 iter 7: train loss 0.21299. lr 5.122639e-04: 100% 8/8 [00:01<00:00,  4.62it/s]\n","epoch 75 iter 7: train loss 0.20300. lr 5.100024e-04: 100% 8/8 [00:01<00:00,  4.66it/s]\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","500it [01:04,  7.73it/s]\n","Correct: 13.0 out of 500.0: 2.6%\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:54,  8.05it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to vanilla.nopretrain.test.predictions; no targets provided\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNphI-jq_g2g","executionInfo":{"status":"ok","timestamp":1621350354193,"user_tz":-540,"elapsed":2448,"user":{"displayName":"우기온앤오프","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLw9XcTXYIvff_IdkIHpgDs6qse_Q2E6OOzd59Ig=s64","userId":"03343870549788815937"}},"outputId":"ffeb299c-27af-4927-d643-7d4c38370c53"},"source":["!python src/london_baseline.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]\r500it [00:00, 624896.31it/s]\n","Correct: 25.0 out of 500.0: 5.0%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIkfevvbA4Xj","executionInfo":{"status":"ok","timestamp":1621350667012,"user_tz":-540,"elapsed":2126,"user":{"displayName":"우기온앤오프","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLw9XcTXYIvff_IdkIHpgDs6qse_Q2E6OOzd59Ig=s64","userId":"03343870549788815937"}},"outputId":"fb3d06e6-a352-45b4-8167-84f0e459d610"},"source":["!python src/dataset.py charcorruption"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data has 418352 characters, 256 unique.\n","x: Khatchig Mouradian.⁇hat⁇ K□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: hatchig Mouradian.⁇hat⁇ K□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Jacob Henry Studer. J⁇b Henry Studer (26 February 1840 Columbus, Ohio - 2 August 19⁇aco□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: acob Henry Studer. J⁇b Henry Studer (26 February 1840 Columbus, Ohio - 2 August 19⁇aco□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: John Stephe⁇a welder's apprentice on leaving school .⁇n. Born in Glasgow, Stephen became □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: ohn Stephe⁇a welder's apprentice on leaving school .⁇n. Born in Glasgow, Stephen became □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Georgin⁇award winning film director who was born in⁇a Willis. Georgina Willis is an □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: eorgin⁇award winning film director who was born in⁇a Willis. Georgina Willis is an □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEAOgKfxU_1P","executionInfo":{"status":"ok","timestamp":1621392755548,"user_tz":-540,"elapsed":1355729,"user":{"displayName":"우기온앤오프","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLw9XcTXYIvff_IdkIHpgDs6qse_Q2E6OOzd59Ig=s64","userId":"03343870549788815937"}},"outputId":"08248878-76c2-4c9c-d92e-8099a0d189a3"},"source":["# Pretrain the model\n","!python src/run.py pretrain vanilla wiki.txt \\\n","--writing_params_path vanilla.pretrain.params\n","# Finetune the model\n","!python src/run.py finetune vanilla wiki.txt \\\n","--reading_params_path vanilla.pretrain.params \\\n","--writing_params_path vanilla.finetune.params \\\n","--finetune_corpus_path birth_places_train.tsv\n","# Evaluate on the dev set; write to disk\n","!python src/run.py evaluate vanilla wiki.txt \\\n","--reading_params_path vanilla.finetune.params \\\n","--eval_corpus_path birth_dev.tsv \\\n","--outputs_path vanilla.pretrain.dev.predictions\n","# Evaluate on the test set; write to disk\n","!python src/run.py evaluate vanilla wiki.txt \\\n","--reading_params_path vanilla.finetune.params \\\n","--eval_corpus_path birth_test_inputs.tsv \\\n","--outputs_path vanilla.pretrain.test.predictions"],"execution_count":5,"outputs":[{"output_type":"stream","text":["data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 22: train loss 3.42034. lr 5.999655e-03: 100% 23/23 [00:01<00:00, 13.99it/s]\n","epoch 2 iter 22: train loss 3.00224. lr 5.998582e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 3 iter 22: train loss 2.84748. lr 5.996780e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 4 iter 22: train loss 2.77919. lr 5.994250e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 5 iter 22: train loss 2.73450. lr 5.990993e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 6 iter 22: train loss 2.66659. lr 5.987009e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 7 iter 22: train loss 2.68121. lr 5.982299e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 8 iter 22: train loss 2.63052. lr 5.976865e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 9 iter 22: train loss 2.59845. lr 5.970707e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 10 iter 22: train loss 2.59629. lr 5.963828e-03: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 11 iter 22: train loss 2.57252. lr 5.956228e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 12 iter 22: train loss 2.54490. lr 5.947911e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 13 iter 22: train loss 2.51860. lr 5.938877e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 14 iter 22: train loss 2.52027. lr 5.929129e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 15 iter 22: train loss 2.50719. lr 5.918669e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 16 iter 22: train loss 2.48812. lr 5.907500e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 17 iter 22: train loss 2.46389. lr 5.895625e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 18 iter 22: train loss 2.45164. lr 5.883046e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 19 iter 22: train loss 2.49030. lr 5.869767e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 20 iter 22: train loss 2.43941. lr 5.855791e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 21 iter 22: train loss 2.43830. lr 5.841120e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 22 iter 22: train loss 2.43042. lr 5.825760e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 23 iter 22: train loss 2.41352. lr 5.809713e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 24 iter 22: train loss 2.40717. lr 5.792983e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 25 iter 22: train loss 2.36341. lr 5.775575e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 26 iter 22: train loss 2.34771. lr 5.757492e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 27 iter 22: train loss 2.36097. lr 5.738739e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 28 iter 22: train loss 2.33190. lr 5.719321e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 29 iter 22: train loss 2.28953. lr 5.699242e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 30 iter 22: train loss 2.27321. lr 5.678508e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 31 iter 22: train loss 2.23269. lr 5.657123e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 32 iter 22: train loss 2.24853. lr 5.635092e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 33 iter 22: train loss 2.23954. lr 5.612421e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 34 iter 22: train loss 2.20829. lr 5.589115e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 35 iter 22: train loss 2.17802. lr 5.565180e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 36 iter 22: train loss 2.14630. lr 5.540622e-03: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 37 iter 22: train loss 2.10513. lr 5.515446e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 38 iter 22: train loss 2.15535. lr 5.489660e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 39 iter 22: train loss 2.10748. lr 5.463268e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 40 iter 22: train loss 2.09832. lr 5.436278e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 41 iter 22: train loss 2.07330. lr 5.408696e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 42 iter 22: train loss 2.07650. lr 5.380529e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 43 iter 22: train loss 2.04299. lr 5.351784e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 44 iter 22: train loss 2.01127. lr 5.322467e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 45 iter 22: train loss 1.97219. lr 5.292586e-03: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 46 iter 22: train loss 1.92527. lr 5.262148e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 47 iter 22: train loss 1.94323. lr 5.231160e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 48 iter 22: train loss 1.86868. lr 5.199630e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 49 iter 22: train loss 1.92042. lr 5.167566e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 50 iter 22: train loss 1.84112. lr 5.134975e-03: 100% 23/23 [00:01<00:00, 14.03it/s]\n","epoch 51 iter 22: train loss 1.81961. lr 5.101865e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 52 iter 22: train loss 1.84798. lr 5.068245e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 53 iter 22: train loss 1.74699. lr 5.034122e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 54 iter 22: train loss 1.76386. lr 4.999505e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 55 iter 22: train loss 1.78004. lr 4.964402e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 56 iter 22: train loss 1.72762. lr 4.928822e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 57 iter 22: train loss 1.71214. lr 4.892774e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 58 iter 22: train loss 1.72982. lr 4.856265e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 59 iter 22: train loss 1.71379. lr 4.819305e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 60 iter 22: train loss 1.69866. lr 4.781904e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 61 iter 22: train loss 1.69217. lr 4.744069e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 62 iter 22: train loss 1.65787. lr 4.705811e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 63 iter 22: train loss 1.64646. lr 4.667138e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 64 iter 22: train loss 1.59832. lr 4.628061e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 65 iter 22: train loss 1.59155. lr 4.588587e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 66 iter 22: train loss 1.55465. lr 4.548728e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 67 iter 22: train loss 1.58017. lr 4.508492e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 68 iter 22: train loss 1.51723. lr 4.467890e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 69 iter 22: train loss 1.54726. lr 4.426932e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 70 iter 22: train loss 1.50206. lr 4.385626e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 71 iter 22: train loss 1.46383. lr 4.343984e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 72 iter 22: train loss 1.51602. lr 4.302016e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 73 iter 22: train loss 1.48716. lr 4.259731e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 74 iter 22: train loss 1.47891. lr 4.217140e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 75 iter 22: train loss 1.45397. lr 4.174253e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 76 iter 22: train loss 1.46596. lr 4.131081e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 77 iter 22: train loss 1.43729. lr 4.087634e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 78 iter 22: train loss 1.37419. lr 4.043923e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 79 iter 22: train loss 1.41873. lr 3.999958e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 80 iter 22: train loss 1.38056. lr 3.955751e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 81 iter 22: train loss 1.36828. lr 3.911311e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 82 iter 22: train loss 1.39486. lr 3.866650e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 83 iter 22: train loss 1.37516. lr 3.821778e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 84 iter 22: train loss 1.38517. lr 3.776706e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 85 iter 22: train loss 1.38699. lr 3.731446e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 86 iter 22: train loss 1.36626. lr 3.686008e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 87 iter 22: train loss 1.31668. lr 3.640404e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 88 iter 22: train loss 1.28823. lr 3.594643e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 89 iter 22: train loss 1.35823. lr 3.548739e-03: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 90 iter 22: train loss 1.30144. lr 3.502701e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 91 iter 22: train loss 1.31751. lr 3.456541e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 92 iter 22: train loss 1.26636. lr 3.410270e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 93 iter 22: train loss 1.25658. lr 3.363899e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 94 iter 22: train loss 1.28094. lr 3.317440e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 95 iter 22: train loss 1.24147. lr 3.270903e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 96 iter 22: train loss 1.22103. lr 3.224301e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 97 iter 22: train loss 1.28438. lr 3.177645e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 98 iter 22: train loss 1.26233. lr 3.130945e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 99 iter 22: train loss 1.23606. lr 3.084213e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 100 iter 22: train loss 1.23069. lr 3.037461e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 101 iter 22: train loss 1.20862. lr 2.990700e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 102 iter 22: train loss 1.20547. lr 2.943941e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 103 iter 22: train loss 1.20367. lr 2.897196e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 104 iter 22: train loss 1.20937. lr 2.850476e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 105 iter 22: train loss 1.14335. lr 2.803792e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 106 iter 22: train loss 1.17764. lr 2.757156e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 107 iter 22: train loss 1.17262. lr 2.710579e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 108 iter 22: train loss 1.15847. lr 2.664072e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 109 iter 22: train loss 1.12776. lr 2.617646e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 110 iter 22: train loss 1.12384. lr 2.571314e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 111 iter 22: train loss 1.14854. lr 2.525086e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 112 iter 22: train loss 1.12520. lr 2.478973e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 113 iter 22: train loss 1.10738. lr 2.432986e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 114 iter 22: train loss 1.14119. lr 2.387138e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 115 iter 22: train loss 1.09798. lr 2.341438e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 116 iter 22: train loss 1.05813. lr 2.295899e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 117 iter 22: train loss 1.07397. lr 2.250530e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 118 iter 22: train loss 1.13005. lr 2.205344e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 119 iter 22: train loss 1.11701. lr 2.160350e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 120 iter 22: train loss 1.05931. lr 2.115561e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 121 iter 22: train loss 1.06564. lr 2.070986e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 122 iter 22: train loss 1.09114. lr 2.026638e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 123 iter 22: train loss 1.05056. lr 1.982525e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 124 iter 22: train loss 1.04108. lr 1.938660e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 125 iter 22: train loss 1.04562. lr 1.895053e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 126 iter 22: train loss 1.03261. lr 1.851714e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 127 iter 22: train loss 1.03000. lr 1.808654e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 128 iter 22: train loss 1.04562. lr 1.765884e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 129 iter 22: train loss 1.02541. lr 1.723414e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 130 iter 22: train loss 1.00026. lr 1.681253e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 131 iter 22: train loss 1.01196. lr 1.639413e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 132 iter 22: train loss 0.96752. lr 1.597904e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 133 iter 22: train loss 1.03565. lr 1.556735e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 134 iter 22: train loss 0.99431. lr 1.515917e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 135 iter 22: train loss 0.94698. lr 1.475460e-03: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 136 iter 22: train loss 1.01076. lr 1.435373e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 137 iter 22: train loss 0.96365. lr 1.395666e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 138 iter 22: train loss 0.99040. lr 1.356349e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 139 iter 22: train loss 0.96421. lr 1.317431e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 140 iter 22: train loss 0.95777. lr 1.278922e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 141 iter 22: train loss 0.96015. lr 1.240831e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 142 iter 22: train loss 0.98148. lr 1.203168e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 143 iter 22: train loss 0.97282. lr 1.165941e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 144 iter 22: train loss 0.95103. lr 1.129159e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 145 iter 22: train loss 0.95725. lr 1.092833e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 146 iter 22: train loss 0.95315. lr 1.056969e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 147 iter 22: train loss 0.93029. lr 1.021578e-03: 100% 23/23 [00:01<00:00, 13.73it/s]\n","epoch 148 iter 22: train loss 0.94429. lr 9.866675e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 149 iter 22: train loss 0.91988. lr 9.522460e-04: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 150 iter 22: train loss 0.92757. lr 9.183221e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 151 iter 22: train loss 0.91695. lr 8.849040e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 152 iter 22: train loss 0.92168. lr 8.519997e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 153 iter 22: train loss 0.91292. lr 8.196174e-04: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 154 iter 22: train loss 0.92018. lr 7.877647e-04: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 155 iter 22: train loss 0.95325. lr 7.564496e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 156 iter 22: train loss 0.88793. lr 7.256796e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 157 iter 22: train loss 0.92315. lr 6.954621e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 158 iter 22: train loss 0.90525. lr 6.658045e-04: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 159 iter 22: train loss 0.91075. lr 6.367141e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 160 iter 22: train loss 0.88715. lr 6.081979e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 161 iter 22: train loss 0.92087. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 162 iter 22: train loss 0.89484. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 163 iter 22: train loss 0.90413. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 164 iter 22: train loss 0.86261. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 165 iter 22: train loss 0.91730. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 166 iter 22: train loss 0.86249. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 167 iter 22: train loss 0.90288. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 168 iter 22: train loss 0.89464. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.03it/s]\n","epoch 169 iter 22: train loss 0.86132. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 170 iter 22: train loss 0.88669. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 171 iter 22: train loss 0.87961. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 172 iter 22: train loss 0.90393. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 173 iter 22: train loss 0.90229. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 174 iter 22: train loss 0.90133. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 175 iter 22: train loss 0.85922. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 176 iter 22: train loss 0.85140. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 177 iter 22: train loss 0.89100. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 178 iter 22: train loss 0.87262. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 179 iter 22: train loss 0.88472. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 180 iter 22: train loss 0.87743. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 181 iter 22: train loss 0.89937. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 182 iter 22: train loss 0.86692. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 183 iter 22: train loss 0.91759. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 184 iter 22: train loss 0.89638. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 185 iter 22: train loss 0.87878. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 186 iter 22: train loss 0.85577. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 187 iter 22: train loss 0.87792. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 188 iter 22: train loss 0.88655. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 189 iter 22: train loss 0.84150. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 190 iter 22: train loss 0.83883. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 191 iter 22: train loss 0.86626. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 192 iter 22: train loss 0.84816. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 193 iter 22: train loss 0.87759. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 194 iter 22: train loss 0.84712. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 195 iter 22: train loss 0.83446. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 196 iter 22: train loss 0.85518. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 197 iter 22: train loss 0.84138. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 198 iter 22: train loss 0.87087. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 199 iter 22: train loss 0.87021. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 200 iter 22: train loss 0.84759. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 201 iter 22: train loss 0.85752. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 202 iter 22: train loss 0.86589. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 203 iter 22: train loss 0.84598. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 204 iter 22: train loss 0.87322. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 205 iter 22: train loss 0.82570. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 206 iter 22: train loss 0.83644. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 207 iter 22: train loss 0.83436. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 208 iter 22: train loss 0.82762. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 209 iter 22: train loss 0.85943. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 210 iter 22: train loss 0.88808. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 211 iter 22: train loss 0.82156. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 212 iter 22: train loss 0.85509. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 213 iter 22: train loss 0.82860. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 214 iter 22: train loss 0.85674. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 215 iter 22: train loss 0.83158. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 216 iter 22: train loss 0.83261. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 217 iter 22: train loss 0.83786. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 218 iter 22: train loss 0.86138. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 219 iter 22: train loss 0.82740. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 220 iter 22: train loss 0.78095. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 221 iter 22: train loss 0.80294. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 222 iter 22: train loss 0.79447. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 223 iter 22: train loss 0.82317. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 224 iter 22: train loss 0.80559. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 225 iter 22: train loss 0.82030. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 226 iter 22: train loss 0.87065. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 227 iter 22: train loss 0.81179. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 228 iter 22: train loss 0.80290. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 229 iter 22: train loss 0.82163. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 230 iter 22: train loss 0.82669. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 231 iter 22: train loss 0.81971. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 232 iter 22: train loss 0.81297. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 233 iter 22: train loss 0.83747. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 234 iter 22: train loss 0.81455. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 235 iter 22: train loss 0.84857. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 236 iter 22: train loss 0.78503. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 237 iter 22: train loss 0.83862. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 238 iter 22: train loss 0.83459. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 239 iter 22: train loss 0.80116. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 240 iter 22: train loss 0.80430. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 241 iter 22: train loss 0.80570. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 242 iter 22: train loss 0.80806. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.85it/s]\n","epoch 243 iter 22: train loss 0.80206. lr 6.039815e-04: 100% 23/23 [00:01<00:00, 13.69it/s]\n","epoch 244 iter 22: train loss 0.81027. lr 6.324112e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 245 iter 22: train loss 0.81923. lr 6.614162e-04: 100% 23/23 [00:01<00:00, 13.76it/s]\n","epoch 246 iter 22: train loss 0.81658. lr 6.909893e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 247 iter 22: train loss 0.79423. lr 7.211235e-04: 100% 23/23 [00:01<00:00, 13.85it/s]\n","epoch 248 iter 22: train loss 0.82668. lr 7.518114e-04: 100% 23/23 [00:01<00:00, 13.46it/s]\n","epoch 249 iter 22: train loss 0.81801. lr 7.830454e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 250 iter 22: train loss 0.81668. lr 8.148181e-04: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 251 iter 22: train loss 0.79752. lr 8.471217e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 252 iter 22: train loss 0.80633. lr 8.799484e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 253 iter 22: train loss 0.82298. lr 9.132902e-04: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 254 iter 22: train loss 0.78356. lr 9.471389e-04: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 255 iter 22: train loss 0.82328. lr 9.814865e-04: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 256 iter 22: train loss 0.81526. lr 1.016324e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 257 iter 22: train loss 0.82029. lr 1.051644e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 258 iter 22: train loss 0.82399. lr 1.087438e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 259 iter 22: train loss 0.82181. lr 1.123696e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 260 iter 22: train loss 0.82880. lr 1.160409e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 261 iter 22: train loss 0.82914. lr 1.197570e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 262 iter 22: train loss 0.81581. lr 1.235169e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 263 iter 22: train loss 0.84566. lr 1.273196e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 264 iter 22: train loss 0.84059. lr 1.311643e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 265 iter 22: train loss 0.82966. lr 1.350501e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 266 iter 22: train loss 0.85208. lr 1.389759e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 267 iter 22: train loss 0.82758. lr 1.429408e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 268 iter 22: train loss 0.84072. lr 1.469439e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 269 iter 22: train loss 0.82558. lr 1.509841e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 270 iter 22: train loss 0.83606. lr 1.550606e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 271 iter 22: train loss 0.83416. lr 1.591723e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 272 iter 22: train loss 0.85661. lr 1.633182e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 273 iter 22: train loss 0.82553. lr 1.674973e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 274 iter 22: train loss 0.85278. lr 1.717086e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 275 iter 22: train loss 0.84491. lr 1.759511e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 276 iter 22: train loss 0.84987. lr 1.802237e-03: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 277 iter 22: train loss 0.83927. lr 1.845254e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 278 iter 22: train loss 0.84865. lr 1.888552e-03: 100% 23/23 [00:01<00:00, 14.01it/s]\n","epoch 279 iter 22: train loss 0.85748. lr 1.932120e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 280 iter 22: train loss 0.85261. lr 1.975947e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 281 iter 22: train loss 0.85126. lr 2.020023e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 282 iter 22: train loss 0.85505. lr 2.064337e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 283 iter 22: train loss 0.85527. lr 2.108878e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 284 iter 22: train loss 0.86638. lr 2.153636e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 285 iter 22: train loss 0.86671. lr 2.198600e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 286 iter 22: train loss 0.88952. lr 2.243758e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 287 iter 22: train loss 0.86725. lr 2.289100e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 288 iter 22: train loss 0.90269. lr 2.334615e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 289 iter 22: train loss 0.86721. lr 2.380291e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 290 iter 22: train loss 0.87374. lr 2.426118e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 291 iter 22: train loss 0.86549. lr 2.472085e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 292 iter 22: train loss 0.87085. lr 2.518179e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 293 iter 22: train loss 0.88843. lr 2.564391e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 294 iter 22: train loss 0.87673. lr 2.610708e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 295 iter 22: train loss 0.92590. lr 2.657121e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 296 iter 22: train loss 0.87836. lr 2.703616e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 297 iter 22: train loss 0.86264. lr 2.750184e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 298 iter 22: train loss 0.90609. lr 2.796812e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 299 iter 22: train loss 0.89735. lr 2.843489e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 300 iter 22: train loss 0.91457. lr 2.890205e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 301 iter 22: train loss 0.91953. lr 2.936947e-03: 100% 23/23 [00:01<00:00, 14.03it/s]\n","epoch 302 iter 22: train loss 0.89281. lr 2.983705e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 303 iter 22: train loss 0.91112. lr 3.030466e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 304 iter 22: train loss 0.89034. lr 3.077220e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 305 iter 22: train loss 0.89350. lr 3.123955e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 306 iter 22: train loss 0.90048. lr 3.170661e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 307 iter 22: train loss 0.88809. lr 3.217324e-03: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 308 iter 22: train loss 0.89820. lr 3.263935e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 309 iter 22: train loss 0.93385. lr 3.310482e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 310 iter 22: train loss 0.91611. lr 3.356954e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 311 iter 22: train loss 0.91162. lr 3.403338e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 312 iter 22: train loss 0.87699. lr 3.449625e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 313 iter 22: train loss 0.92718. lr 3.495802e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 314 iter 22: train loss 0.94421. lr 3.541859e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 315 iter 22: train loss 0.91741. lr 3.587785e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 316 iter 22: train loss 0.88741. lr 3.633567e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 317 iter 22: train loss 0.93433. lr 3.679196e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 318 iter 22: train loss 0.91289. lr 3.724659e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 319 iter 22: train loss 0.91318. lr 3.769947e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 320 iter 22: train loss 0.92672. lr 3.815047e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 321 iter 22: train loss 0.93733. lr 3.859950e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 322 iter 22: train loss 0.92102. lr 3.904643e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 323 iter 22: train loss 0.94090. lr 3.949117e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 324 iter 22: train loss 0.90180. lr 3.993360e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 325 iter 22: train loss 0.93015. lr 4.037362e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 326 iter 22: train loss 0.93457. lr 4.081111e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 327 iter 22: train loss 0.95103. lr 4.124598e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 328 iter 22: train loss 0.93960. lr 4.167812e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 329 iter 22: train loss 0.93768. lr 4.210742e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 330 iter 22: train loss 0.91726. lr 4.253378e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 331 iter 22: train loss 0.94303. lr 4.295709e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 332 iter 22: train loss 0.94567. lr 4.337726e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 333 iter 22: train loss 0.93306. lr 4.379418e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 334 iter 22: train loss 0.92324. lr 4.420774e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 335 iter 22: train loss 0.94217. lr 4.461785e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 336 iter 22: train loss 0.91097. lr 4.502441e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 337 iter 22: train loss 0.95438. lr 4.542732e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 338 iter 22: train loss 0.96024. lr 4.582648e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 339 iter 22: train loss 0.93488. lr 4.622180e-03: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 340 iter 22: train loss 0.94336. lr 4.661318e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 341 iter 22: train loss 0.92702. lr 4.700052e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 342 iter 22: train loss 0.91028. lr 4.738372e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 343 iter 22: train loss 0.93116. lr 4.776271e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 344 iter 22: train loss 0.95528. lr 4.813738e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 345 iter 22: train loss 0.94820. lr 4.850764e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 346 iter 22: train loss 0.88940. lr 4.887341e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 347 iter 22: train loss 0.96527. lr 4.923459e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 348 iter 22: train loss 0.93457. lr 4.959110e-03: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 349 iter 22: train loss 0.94203. lr 4.994284e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 350 iter 22: train loss 0.91401. lr 5.028975e-03: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 351 iter 22: train loss 0.93562. lr 5.063172e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 352 iter 22: train loss 0.95111. lr 5.096868e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 353 iter 22: train loss 0.89337. lr 5.130054e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 354 iter 22: train loss 0.92673. lr 5.162723e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 355 iter 22: train loss 0.91265. lr 5.194867e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 356 iter 22: train loss 0.91613. lr 5.226477e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 357 iter 22: train loss 0.95229. lr 5.257546e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 358 iter 22: train loss 0.92560. lr 5.288067e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 359 iter 22: train loss 0.92445. lr 5.318032e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 360 iter 22: train loss 0.90906. lr 5.347434e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 361 iter 22: train loss 0.90682. lr 5.376265e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 362 iter 22: train loss 0.96014. lr 5.404519e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 363 iter 22: train loss 0.94554. lr 5.432189e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 364 iter 22: train loss 0.90233. lr 5.459268e-03: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 365 iter 22: train loss 0.93528. lr 5.485750e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 366 iter 22: train loss 0.90962. lr 5.511627e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 367 iter 22: train loss 0.89917. lr 5.536894e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 368 iter 22: train loss 0.90065. lr 5.561545e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 369 iter 22: train loss 0.95201. lr 5.585574e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 370 iter 22: train loss 0.92995. lr 5.608974e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 371 iter 22: train loss 0.89371. lr 5.631741e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 372 iter 22: train loss 0.92046. lr 5.653868e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 373 iter 22: train loss 0.89665. lr 5.675350e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 374 iter 22: train loss 0.91988. lr 5.696182e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 375 iter 22: train loss 0.89967. lr 5.716359e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 376 iter 22: train loss 0.89425. lr 5.735876e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 377 iter 22: train loss 0.91123. lr 5.754729e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 378 iter 22: train loss 0.91400. lr 5.772912e-03: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 379 iter 22: train loss 0.90816. lr 5.790422e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 380 iter 22: train loss 0.89293. lr 5.807253e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 381 iter 22: train loss 0.92202. lr 5.823403e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 382 iter 22: train loss 0.90259. lr 5.838866e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 383 iter 22: train loss 0.92539. lr 5.853640e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 384 iter 22: train loss 0.90701. lr 5.867720e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 385 iter 22: train loss 0.89832. lr 5.881104e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 386 iter 22: train loss 0.93104. lr 5.893788e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 387 iter 22: train loss 0.87978. lr 5.905768e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 388 iter 22: train loss 0.90999. lr 5.917043e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 389 iter 22: train loss 0.93189. lr 5.927609e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 390 iter 22: train loss 0.92455. lr 5.937464e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 391 iter 22: train loss 0.91133. lr 5.946605e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 392 iter 22: train loss 0.87757. lr 5.955030e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 393 iter 22: train loss 0.87426. lr 5.962737e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 394 iter 22: train loss 0.87211. lr 5.969724e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 395 iter 22: train loss 0.89259. lr 5.975990e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 396 iter 22: train loss 0.91985. lr 5.981532e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 397 iter 22: train loss 0.88300. lr 5.986351e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 398 iter 22: train loss 0.84565. lr 5.990443e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 399 iter 22: train loss 0.83512. lr 5.993809e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 400 iter 22: train loss 0.88490. lr 5.996448e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 401 iter 22: train loss 0.87387. lr 5.998359e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 402 iter 22: train loss 0.88895. lr 5.999541e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 403 iter 22: train loss 0.83828. lr 5.999995e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 404 iter 22: train loss 0.85531. lr 5.999719e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 405 iter 22: train loss 0.86850. lr 5.998715e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 406 iter 22: train loss 0.83407. lr 5.996982e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 407 iter 22: train loss 0.91086. lr 5.994521e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 408 iter 22: train loss 0.90377. lr 5.991333e-03: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 409 iter 22: train loss 0.88469. lr 5.987417e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 410 iter 22: train loss 0.86274. lr 5.982776e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 411 iter 22: train loss 0.84720. lr 5.977411e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 412 iter 22: train loss 0.83222. lr 5.971321e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 413 iter 22: train loss 0.86173. lr 5.964510e-03: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 414 iter 22: train loss 0.87142. lr 5.956979e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 415 iter 22: train loss 0.87022. lr 5.948729e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 416 iter 22: train loss 0.82856. lr 5.939763e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 417 iter 22: train loss 0.81172. lr 5.930082e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 418 iter 22: train loss 0.86487. lr 5.919690e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 419 iter 22: train loss 0.84690. lr 5.908588e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 420 iter 22: train loss 0.83290. lr 5.896780e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 421 iter 22: train loss 0.84071. lr 5.884268e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 422 iter 22: train loss 0.83059. lr 5.871055e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 423 iter 22: train loss 0.86405. lr 5.857144e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 424 iter 22: train loss 0.84337. lr 5.842539e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 425 iter 22: train loss 0.84607. lr 5.827244e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 426 iter 22: train loss 0.80550. lr 5.811262e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 427 iter 22: train loss 0.81408. lr 5.794596e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 428 iter 22: train loss 0.83866. lr 5.777252e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 429 iter 22: train loss 0.82644. lr 5.759233e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 430 iter 22: train loss 0.83516. lr 5.740544e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 431 iter 22: train loss 0.82225. lr 5.721189e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 432 iter 22: train loss 0.81287. lr 5.701172e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 433 iter 22: train loss 0.83476. lr 5.680499e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 434 iter 22: train loss 0.81824. lr 5.659176e-03: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 435 iter 22: train loss 0.83575. lr 5.637206e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 436 iter 22: train loss 0.80497. lr 5.614595e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 437 iter 22: train loss 0.80662. lr 5.591349e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 438 iter 22: train loss 0.84081. lr 5.567473e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 439 iter 22: train loss 0.75837. lr 5.542974e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 440 iter 22: train loss 0.81134. lr 5.517857e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 441 iter 22: train loss 0.79939. lr 5.492128e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 442 iter 22: train loss 0.81199. lr 5.465793e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 443 iter 22: train loss 0.76818. lr 5.438860e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 444 iter 22: train loss 0.78940. lr 5.411333e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 445 iter 22: train loss 0.79079. lr 5.383222e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 446 iter 22: train loss 0.78633. lr 5.354531e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 447 iter 22: train loss 0.80788. lr 5.325267e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 448 iter 22: train loss 0.76846. lr 5.295439e-03: 100% 23/23 [00:01<00:00, 14.03it/s]\n","epoch 449 iter 22: train loss 0.79627. lr 5.265054e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 450 iter 22: train loss 0.76160. lr 5.234118e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 451 iter 22: train loss 0.76803. lr 5.202639e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 452 iter 22: train loss 0.78673. lr 5.170625e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 453 iter 22: train loss 0.82468. lr 5.138084e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 454 iter 22: train loss 0.76996. lr 5.105023e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 455 iter 22: train loss 0.77677. lr 5.071450e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 456 iter 22: train loss 0.76223. lr 5.037375e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 457 iter 22: train loss 0.76275. lr 5.002804e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 458 iter 22: train loss 0.76584. lr 4.967747e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 459 iter 22: train loss 0.74901. lr 4.932212e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 460 iter 22: train loss 0.75179. lr 4.896207e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 461 iter 22: train loss 0.76522. lr 4.859742e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 462 iter 22: train loss 0.75303. lr 4.822825e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 463 iter 22: train loss 0.73022. lr 4.785465e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 464 iter 22: train loss 0.73824. lr 4.747671e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 465 iter 22: train loss 0.74223. lr 4.709452e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 466 iter 22: train loss 0.72827. lr 4.670818e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 467 iter 22: train loss 0.73942. lr 4.631778e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 468 iter 22: train loss 0.75653. lr 4.592342e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 469 iter 22: train loss 0.72962. lr 4.552519e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 470 iter 22: train loss 0.72967. lr 4.512319e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 471 iter 22: train loss 0.74842. lr 4.471751e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 472 iter 22: train loss 0.72136. lr 4.430825e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 473 iter 22: train loss 0.72993. lr 4.389552e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 474 iter 22: train loss 0.77662. lr 4.347942e-03: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 475 iter 22: train loss 0.71418. lr 4.306004e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 476 iter 22: train loss 0.70427. lr 4.263748e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 477 iter 22: train loss 0.73056. lr 4.221186e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 478 iter 22: train loss 0.71564. lr 4.178327e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 479 iter 22: train loss 0.71076. lr 4.135181e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 480 iter 22: train loss 0.69869. lr 4.091760e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 481 iter 22: train loss 0.69780. lr 4.048074e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 482 iter 22: train loss 0.69105. lr 4.004132e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 483 iter 22: train loss 0.69759. lr 3.959947e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 484 iter 22: train loss 0.72782. lr 3.915529e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 485 iter 22: train loss 0.70541. lr 3.870888e-03: 100% 23/23 [00:01<00:00, 13.92it/s]\n","epoch 486 iter 22: train loss 0.69322. lr 3.826036e-03: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 487 iter 22: train loss 0.68674. lr 3.780983e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 488 iter 22: train loss 0.69027. lr 3.735740e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 489 iter 22: train loss 0.67701. lr 3.690318e-03: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 490 iter 22: train loss 0.67343. lr 3.644729e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 491 iter 22: train loss 0.67271. lr 3.598983e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 492 iter 22: train loss 0.71362. lr 3.553092e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 493 iter 22: train loss 0.67473. lr 3.507066e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 494 iter 22: train loss 0.63420. lr 3.460917e-03: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 495 iter 22: train loss 0.63143. lr 3.414656e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 496 iter 22: train loss 0.66281. lr 3.368294e-03: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 497 iter 22: train loss 0.67036. lr 3.321843e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 498 iter 22: train loss 0.62519. lr 3.275313e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 499 iter 22: train loss 0.65342. lr 3.228717e-03: 100% 23/23 [00:01<00:00, 13.89it/s]\n","epoch 500 iter 22: train loss 0.64397. lr 3.182065e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 501 iter 22: train loss 0.63433. lr 3.135369e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 502 iter 22: train loss 0.63752. lr 3.088640e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 503 iter 22: train loss 0.63964. lr 3.041889e-03: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 504 iter 22: train loss 0.65341. lr 2.995129e-03: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 505 iter 22: train loss 0.61774. lr 2.948369e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 506 iter 22: train loss 0.62227. lr 2.901622e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 507 iter 22: train loss 0.62039. lr 2.854899e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 508 iter 22: train loss 0.61764. lr 2.808211e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 509 iter 22: train loss 0.62317. lr 2.761570e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 510 iter 22: train loss 0.63043. lr 2.714987e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 511 iter 22: train loss 0.61028. lr 2.668473e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 512 iter 22: train loss 0.59821. lr 2.622039e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 513 iter 22: train loss 0.62307. lr 2.575697e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 514 iter 22: train loss 0.60310. lr 2.529459e-03: 100% 23/23 [00:01<00:00, 13.94it/s]\n","epoch 515 iter 22: train loss 0.63899. lr 2.483334e-03: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 516 iter 22: train loss 0.59052. lr 2.437336e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 517 iter 22: train loss 0.64282. lr 2.391474e-03: 100% 23/23 [00:01<00:00, 14.01it/s]\n","epoch 518 iter 22: train loss 0.59506. lr 2.345759e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 519 iter 22: train loss 0.58731. lr 2.300204e-03: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 520 iter 22: train loss 0.61329. lr 2.254819e-03: 100% 23/23 [00:01<00:00, 13.98it/s]\n","epoch 521 iter 22: train loss 0.58424. lr 2.209615e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 522 iter 22: train loss 0.58420. lr 2.164603e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 523 iter 22: train loss 0.58905. lr 2.119793e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 524 iter 22: train loss 0.58096. lr 2.075198e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 525 iter 22: train loss 0.58528. lr 2.030827e-03: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 526 iter 22: train loss 0.58282. lr 1.986692e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 527 iter 22: train loss 0.56095. lr 1.942803e-03: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 528 iter 22: train loss 0.58179. lr 1.899171e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 529 iter 22: train loss 0.60020. lr 1.855807e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 530 iter 22: train loss 0.56450. lr 1.812720e-03: 100% 23/23 [00:01<00:00, 13.89it/s]\n","epoch 531 iter 22: train loss 0.60292. lr 1.769922e-03: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 532 iter 22: train loss 0.56715. lr 1.727422e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 533 iter 22: train loss 0.56785. lr 1.685232e-03: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 534 iter 22: train loss 0.55165. lr 1.643361e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 535 iter 22: train loss 0.55406. lr 1.601820e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 536 iter 22: train loss 0.57109. lr 1.560619e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 537 iter 22: train loss 0.58680. lr 1.519767e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 538 iter 22: train loss 0.55573. lr 1.479275e-03: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 539 iter 22: train loss 0.57617. lr 1.439153e-03: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 540 iter 22: train loss 0.55063. lr 1.399409e-03: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 541 iter 22: train loss 0.56469. lr 1.360055e-03: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 542 iter 22: train loss 0.56064. lr 1.321099e-03: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 543 iter 22: train loss 0.52885. lr 1.282551e-03: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 544 iter 22: train loss 0.56053. lr 1.244420e-03: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 545 iter 22: train loss 0.55296. lr 1.206716e-03: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 546 iter 22: train loss 0.52737. lr 1.169447e-03: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 547 iter 22: train loss 0.52318. lr 1.132623e-03: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 548 iter 22: train loss 0.51277. lr 1.096253e-03: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 549 iter 22: train loss 0.53636. lr 1.060345e-03: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 550 iter 22: train loss 0.52505. lr 1.024909e-03: 100% 23/23 [00:01<00:00, 13.93it/s]\n","epoch 551 iter 22: train loss 0.53390. lr 9.899527e-04: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 552 iter 22: train loss 0.53725. lr 9.554845e-04: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 553 iter 22: train loss 0.53068. lr 9.215132e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 554 iter 22: train loss 0.51891. lr 8.880468e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 555 iter 22: train loss 0.53200. lr 8.550935e-04: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 556 iter 22: train loss 0.50816. lr 8.226614e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 557 iter 22: train loss 0.54649. lr 7.907583e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 558 iter 22: train loss 0.50912. lr 7.593919e-04: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 559 iter 22: train loss 0.50422. lr 7.285699e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 560 iter 22: train loss 0.52966. lr 6.982998e-04: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 561 iter 22: train loss 0.51335. lr 6.685889e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 562 iter 22: train loss 0.50179. lr 6.394445e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 563 iter 22: train loss 0.49226. lr 6.108735e-04: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 564 iter 22: train loss 0.50129. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 565 iter 22: train loss 0.48679. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 566 iter 22: train loss 0.52469. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.13it/s]\n","epoch 567 iter 22: train loss 0.51699. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 568 iter 22: train loss 0.51781. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.01it/s]\n","epoch 569 iter 22: train loss 0.53834. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 570 iter 22: train loss 0.52301. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 571 iter 22: train loss 0.49140. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.01it/s]\n","epoch 572 iter 22: train loss 0.47522. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 573 iter 22: train loss 0.50661. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 574 iter 22: train loss 0.50239. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 575 iter 22: train loss 0.48937. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 576 iter 22: train loss 0.47950. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.96it/s]\n","epoch 577 iter 22: train loss 0.49438. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 578 iter 22: train loss 0.51987. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 579 iter 22: train loss 0.50822. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 580 iter 22: train loss 0.52742. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 581 iter 22: train loss 0.49466. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 582 iter 22: train loss 0.50112. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 583 iter 22: train loss 0.49866. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 584 iter 22: train loss 0.50284. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.04it/s]\n","epoch 585 iter 22: train loss 0.47516. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.97it/s]\n","epoch 586 iter 22: train loss 0.48540. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.99it/s]\n","epoch 587 iter 22: train loss 0.47341. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 588 iter 22: train loss 0.50989. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.03it/s]\n","epoch 589 iter 22: train loss 0.51203. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 590 iter 22: train loss 0.47796. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 591 iter 22: train loss 0.46538. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 592 iter 22: train loss 0.47293. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 593 iter 22: train loss 0.48106. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 594 iter 22: train loss 0.50384. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 595 iter 22: train loss 0.52259. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 596 iter 22: train loss 0.50131. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 597 iter 22: train loss 0.44009. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.01it/s]\n","epoch 598 iter 22: train loss 0.48774. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 599 iter 22: train loss 0.49071. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.03it/s]\n","epoch 600 iter 22: train loss 0.48160. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 601 iter 22: train loss 0.47450. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 602 iter 22: train loss 0.47693. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 603 iter 22: train loss 0.52153. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 604 iter 22: train loss 0.53785. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 605 iter 22: train loss 0.47939. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 606 iter 22: train loss 0.49881. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 607 iter 22: train loss 0.49399. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 608 iter 22: train loss 0.48828. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 609 iter 22: train loss 0.50647. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 610 iter 22: train loss 0.48707. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 611 iter 22: train loss 0.47972. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 612 iter 22: train loss 0.47896. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 613 iter 22: train loss 0.47971. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.98it/s]\n","epoch 614 iter 22: train loss 0.49153. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 615 iter 22: train loss 0.49962. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 616 iter 22: train loss 0.47475. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 617 iter 22: train loss 0.48533. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.97it/s]\n","epoch 618 iter 22: train loss 0.46481. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 619 iter 22: train loss 0.47793. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 620 iter 22: train loss 0.48268. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 621 iter 22: train loss 0.50579. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 622 iter 22: train loss 0.49593. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 623 iter 22: train loss 0.47907. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 624 iter 22: train loss 0.49612. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 625 iter 22: train loss 0.48560. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.03it/s]\n","epoch 626 iter 22: train loss 0.47330. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.09it/s]\n","epoch 627 iter 22: train loss 0.48713. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.97it/s]\n","epoch 628 iter 22: train loss 0.48869. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.06it/s]\n","epoch 629 iter 22: train loss 0.50778. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 630 iter 22: train loss 0.48393. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.16it/s]\n","epoch 631 iter 22: train loss 0.48751. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 632 iter 22: train loss 0.49960. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.08it/s]\n","epoch 633 iter 22: train loss 0.50080. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.02it/s]\n","epoch 634 iter 22: train loss 0.48865. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 13.67it/s]\n","epoch 635 iter 22: train loss 0.48635. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.21it/s]\n","epoch 636 iter 22: train loss 0.47118. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 637 iter 22: train loss 0.47172. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.15it/s]\n","epoch 638 iter 22: train loss 0.49391. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.10it/s]\n","epoch 639 iter 22: train loss 0.47335. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.05it/s]\n","epoch 640 iter 22: train loss 0.46669. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 641 iter 22: train loss 0.48539. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 642 iter 22: train loss 0.48152. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 643 iter 22: train loss 0.49352. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 644 iter 22: train loss 0.48615. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 645 iter 22: train loss 0.48190. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.12it/s]\n","epoch 646 iter 22: train loss 0.47973. lr 6.013192e-04: 100% 23/23 [00:01<00:00, 13.94it/s]\n","epoch 647 iter 22: train loss 0.47099. lr 6.296941e-04: 100% 23/23 [00:01<00:00, 14.11it/s]\n","epoch 648 iter 22: train loss 0.47477. lr 6.586449e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","epoch 649 iter 22: train loss 0.47527. lr 6.881646e-04: 100% 23/23 [00:01<00:00, 14.07it/s]\n","epoch 650 iter 22: train loss 0.49598. lr 7.182460e-04: 100% 23/23 [00:01<00:00, 14.14it/s]\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 7: train loss 0.73599. lr 5.999844e-04: 100% 8/8 [00:01<00:00,  7.79it/s]\n","epoch 2 iter 7: train loss 0.57699. lr 5.999351e-04: 100% 8/8 [00:00<00:00,  8.12it/s]\n","epoch 3 iter 7: train loss 0.46580. lr 5.998520e-04: 100% 8/8 [00:00<00:00,  8.16it/s]\n","epoch 4 iter 7: train loss 0.40575. lr 5.997351e-04: 100% 8/8 [00:01<00:00,  7.96it/s]\n","epoch 5 iter 7: train loss 0.34858. lr 5.995844e-04: 100% 8/8 [00:00<00:00,  8.06it/s]\n","epoch 6 iter 7: train loss 0.29833. lr 5.993999e-04: 100% 8/8 [00:00<00:00,  8.28it/s]\n","epoch 7 iter 7: train loss 0.24325. lr 5.991818e-04: 100% 8/8 [00:00<00:00,  8.09it/s]\n","epoch 8 iter 7: train loss 0.20250. lr 5.989299e-04: 100% 8/8 [00:01<00:00,  8.00it/s]\n","epoch 9 iter 7: train loss 0.15718. lr 5.986444e-04: 100% 8/8 [00:00<00:00,  8.14it/s]\n","epoch 10 iter 7: train loss 0.13825. lr 5.983252e-04: 100% 8/8 [00:01<00:00,  7.95it/s]\n","epoch 11 iter 7: train loss 0.12818. lr 5.979723e-04: 100% 8/8 [00:00<00:00,  8.13it/s]\n","epoch 12 iter 7: train loss 0.10082. lr 5.975860e-04: 100% 8/8 [00:00<00:00,  8.06it/s]\n","epoch 13 iter 7: train loss 0.08754. lr 5.971660e-04: 100% 8/8 [00:00<00:00,  8.08it/s]\n","epoch 14 iter 7: train loss 0.07719. lr 5.967127e-04: 100% 8/8 [00:01<00:00,  7.83it/s]\n","epoch 15 iter 7: train loss 0.06240. lr 5.962258e-04: 100% 8/8 [00:00<00:00,  8.04it/s]\n","epoch 16 iter 7: train loss 0.05925. lr 5.957056e-04: 100% 8/8 [00:00<00:00,  8.04it/s]\n","epoch 17 iter 7: train loss 0.05712. lr 5.951521e-04: 100% 8/8 [00:00<00:00,  8.04it/s]\n","epoch 18 iter 7: train loss 0.04919. lr 5.945654e-04: 100% 8/8 [00:00<00:00,  8.14it/s]\n","epoch 19 iter 7: train loss 0.04518. lr 5.939454e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 20 iter 7: train loss 0.03801. lr 5.932923e-04: 100% 8/8 [00:00<00:00,  8.11it/s]\n","epoch 21 iter 7: train loss 0.04165. lr 5.926062e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 22 iter 7: train loss 0.03996. lr 5.918871e-04: 100% 8/8 [00:00<00:00,  8.02it/s]\n","epoch 23 iter 7: train loss 0.03131. lr 5.911352e-04: 100% 8/8 [00:00<00:00,  8.09it/s]\n","epoch 24 iter 7: train loss 0.02813. lr 5.903504e-04: 100% 8/8 [00:00<00:00,  8.12it/s]\n","epoch 25 iter 7: train loss 0.02458. lr 5.895329e-04: 100% 8/8 [00:00<00:00,  8.08it/s]\n","epoch 26 iter 7: train loss 0.02770. lr 5.886828e-04: 100% 8/8 [00:00<00:00,  8.05it/s]\n","epoch 27 iter 7: train loss 0.02208. lr 5.878002e-04: 100% 8/8 [00:00<00:00,  8.05it/s]\n","epoch 28 iter 7: train loss 0.02948. lr 5.868851e-04: 100% 8/8 [00:01<00:00,  7.99it/s]\n","epoch 29 iter 7: train loss 0.01681. lr 5.859378e-04: 100% 8/8 [00:00<00:00,  8.12it/s]\n","epoch 30 iter 7: train loss 0.02303. lr 5.849582e-04: 100% 8/8 [00:00<00:00,  8.20it/s]\n","epoch 31 iter 7: train loss 0.02252. lr 5.839465e-04: 100% 8/8 [00:00<00:00,  8.00it/s]\n","epoch 32 iter 7: train loss 0.02163. lr 5.829028e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 33 iter 7: train loss 0.02017. lr 5.818272e-04: 100% 8/8 [00:00<00:00,  8.00it/s]\n","epoch 34 iter 7: train loss 0.01462. lr 5.807199e-04: 100% 8/8 [00:01<00:00,  7.99it/s]\n","epoch 35 iter 7: train loss 0.02026. lr 5.795810e-04: 100% 8/8 [00:00<00:00,  8.20it/s]\n","epoch 36 iter 7: train loss 0.02011. lr 5.784106e-04: 100% 8/8 [00:00<00:00,  8.06it/s]\n","epoch 37 iter 7: train loss 0.02001. lr 5.772087e-04: 100% 8/8 [00:00<00:00,  8.05it/s]\n","epoch 38 iter 7: train loss 0.01200. lr 5.759757e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 39 iter 7: train loss 0.01361. lr 5.747116e-04: 100% 8/8 [00:01<00:00,  7.97it/s]\n","epoch 40 iter 7: train loss 0.01628. lr 5.734165e-04: 100% 8/8 [00:01<00:00,  7.96it/s]\n","epoch 41 iter 7: train loss 0.01341. lr 5.720906e-04: 100% 8/8 [00:00<00:00,  8.02it/s]\n","epoch 42 iter 7: train loss 0.01513. lr 5.707341e-04: 100% 8/8 [00:00<00:00,  8.04it/s]\n","epoch 43 iter 7: train loss 0.01234. lr 5.693470e-04: 100% 8/8 [00:01<00:00,  7.99it/s]\n","epoch 44 iter 7: train loss 0.00823. lr 5.679296e-04: 100% 8/8 [00:00<00:00,  8.05it/s]\n","epoch 45 iter 7: train loss 0.01800. lr 5.664820e-04: 100% 8/8 [00:00<00:00,  8.06it/s]\n","epoch 46 iter 7: train loss 0.01306. lr 5.650044e-04: 100% 8/8 [00:00<00:00,  8.02it/s]\n","epoch 47 iter 7: train loss 0.01484. lr 5.634970e-04: 100% 8/8 [00:00<00:00,  8.02it/s]\n","epoch 48 iter 7: train loss 0.01097. lr 5.619598e-04: 100% 8/8 [00:00<00:00,  8.08it/s]\n","epoch 49 iter 7: train loss 0.00902. lr 5.603932e-04: 100% 8/8 [00:00<00:00,  8.03it/s]\n","epoch 50 iter 7: train loss 0.01306. lr 5.587972e-04: 100% 8/8 [00:00<00:00,  8.08it/s]\n","epoch 51 iter 7: train loss 0.00956. lr 5.571720e-04: 100% 8/8 [00:00<00:00,  8.07it/s]\n","epoch 52 iter 7: train loss 0.00924. lr 5.555179e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 53 iter 7: train loss 0.00906. lr 5.538350e-04: 100% 8/8 [00:00<00:00,  8.07it/s]\n","epoch 54 iter 7: train loss 0.01085. lr 5.521235e-04: 100% 8/8 [00:01<00:00,  7.97it/s]\n","epoch 55 iter 7: train loss 0.01138. lr 5.503835e-04: 100% 8/8 [00:00<00:00,  8.07it/s]\n","epoch 56 iter 7: train loss 0.00997. lr 5.486154e-04: 100% 8/8 [00:00<00:00,  8.04it/s]\n","epoch 57 iter 7: train loss 0.01244. lr 5.468193e-04: 100% 8/8 [00:01<00:00,  7.99it/s]\n","epoch 58 iter 7: train loss 0.00521. lr 5.449953e-04: 100% 8/8 [00:00<00:00,  8.07it/s]\n","epoch 59 iter 7: train loss 0.00802. lr 5.431438e-04: 100% 8/8 [00:01<00:00,  7.94it/s]\n","epoch 60 iter 7: train loss 0.01139. lr 5.412648e-04: 100% 8/8 [00:00<00:00,  8.17it/s]\n","epoch 61 iter 7: train loss 0.00765. lr 5.393587e-04: 100% 8/8 [00:00<00:00,  8.02it/s]\n","epoch 62 iter 7: train loss 0.00934. lr 5.374256e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 63 iter 7: train loss 0.01023. lr 5.354657e-04: 100% 8/8 [00:01<00:00,  7.92it/s]\n","epoch 64 iter 7: train loss 0.01392. lr 5.334794e-04: 100% 8/8 [00:00<00:00,  8.07it/s]\n","epoch 65 iter 7: train loss 0.00735. lr 5.314667e-04: 100% 8/8 [00:00<00:00,  8.12it/s]\n","epoch 66 iter 7: train loss 0.01075. lr 5.294279e-04: 100% 8/8 [00:00<00:00,  8.13it/s]\n","epoch 67 iter 7: train loss 0.00572. lr 5.273633e-04: 100% 8/8 [00:00<00:00,  8.03it/s]\n","epoch 68 iter 7: train loss 0.00749. lr 5.252731e-04: 100% 8/8 [00:00<00:00,  8.16it/s]\n","epoch 69 iter 7: train loss 0.00736. lr 5.231575e-04: 100% 8/8 [00:00<00:00,  8.17it/s]\n","epoch 70 iter 7: train loss 0.01508. lr 5.210167e-04: 100% 8/8 [00:00<00:00,  8.06it/s]\n","epoch 71 iter 7: train loss 0.00885. lr 5.188511e-04: 100% 8/8 [00:01<00:00,  7.99it/s]\n","epoch 72 iter 7: train loss 0.00788. lr 5.166608e-04: 100% 8/8 [00:01<00:00,  7.94it/s]\n","epoch 73 iter 7: train loss 0.00531. lr 5.144461e-04: 100% 8/8 [00:00<00:00,  8.00it/s]\n","epoch 74 iter 7: train loss 0.00668. lr 5.122072e-04: 100% 8/8 [00:00<00:00,  8.16it/s]\n","epoch 75 iter 7: train loss 0.00750. lr 5.099444e-04: 100% 8/8 [00:01<00:00,  7.95it/s]\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","500it [00:44, 11.15it/s]\n","Correct: 63.0 out of 500.0: 12.6%\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:38, 11.35it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to vanilla.pretrain.test.predictions; no targets provided\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJe_1nUglkac","executionInfo":{"status":"ok","timestamp":1621400836440,"user_tz":-540,"elapsed":1340634,"user":{"displayName":"우기온앤오프","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLw9XcTXYIvff_IdkIHpgDs6qse_Q2E6OOzd59Ig=s64","userId":"03343870549788815937"}},"outputId":"78421329-b945-4699-a532-25b02ff04493"},"source":["# Pretrain the model\n","!python src/run.py pretrain synthesizer wiki.txt \\\n","--writing_params_path synthesizer.pretrain.params\n","# Finetune the model\n","!python src/run.py finetune synthesizer wiki.txt \\\n","--reading_params_path synthesizer.pretrain.params \\\n","--writing_params_path synthesizer.finetune.params \\\n","--finetune_corpus_path birth_places_train.tsv\n","# # # Evaluate on the dev set; write to disk\n","!python src/run.py evaluate synthesizer wiki.txt \\\n","--reading_params_path synthesizer.finetune.params \\\n","--eval_corpus_path birth_dev.tsv \\\n","--outputs_path synthesizer.pretrain.dev.predictions\n","# # # Evaluate on the test set; write to disk\n","!python src/run.py evaluate synthesizer wiki.txt \\\n","--reading_params_path synthesizer.finetune.params \\\n","--eval_corpus_path birth_test_inputs.tsv \\\n","--outputs_path synthesizer.pretrain.test.predictions"],"execution_count":23,"outputs":[{"output_type":"stream","text":["data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","epoch 1 iter 22: train loss 3.44696. lr 5.999655e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 2 iter 22: train loss 3.12128. lr 5.998582e-03: 100% 23/23 [00:01<00:00, 14.62it/s]\n","epoch 3 iter 22: train loss 2.96702. lr 5.996780e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 4 iter 22: train loss 2.85101. lr 5.994250e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 5 iter 22: train loss 2.80488. lr 5.990993e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 6 iter 22: train loss 2.74059. lr 5.987009e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 7 iter 22: train loss 2.71398. lr 5.982299e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 8 iter 22: train loss 2.67478. lr 5.976865e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 9 iter 22: train loss 2.60725. lr 5.970707e-03: 100% 23/23 [00:01<00:00, 14.65it/s]\n","epoch 10 iter 22: train loss 2.55763. lr 5.963828e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 11 iter 22: train loss 2.51104. lr 5.956228e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 12 iter 22: train loss 2.50256. lr 5.947911e-03: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 13 iter 22: train loss 2.44863. lr 5.938877e-03: 100% 23/23 [00:01<00:00, 14.67it/s]\n","epoch 14 iter 22: train loss 2.38140. lr 5.929129e-03: 100% 23/23 [00:01<00:00, 14.62it/s]\n","epoch 15 iter 22: train loss 2.33770. lr 5.918669e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 16 iter 22: train loss 2.27205. lr 5.907500e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 17 iter 22: train loss 2.23537. lr 5.895625e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 18 iter 22: train loss 2.16197. lr 5.883046e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 19 iter 22: train loss 2.09561. lr 5.869767e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 20 iter 22: train loss 2.07503. lr 5.855791e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 21 iter 22: train loss 2.04086. lr 5.841120e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 22 iter 22: train loss 1.97749. lr 5.825760e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 23 iter 22: train loss 1.97100. lr 5.809713e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 24 iter 22: train loss 1.90258. lr 5.792983e-03: 100% 23/23 [00:01<00:00, 14.67it/s]\n","epoch 25 iter 22: train loss 1.87398. lr 5.775575e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 26 iter 22: train loss 1.90844. lr 5.757492e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 27 iter 22: train loss 1.82386. lr 5.738739e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 28 iter 22: train loss 1.76486. lr 5.719321e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 29 iter 22: train loss 1.72599. lr 5.699242e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 30 iter 22: train loss 1.71959. lr 5.678508e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 31 iter 22: train loss 1.73425. lr 5.657123e-03: 100% 23/23 [00:01<00:00, 14.67it/s]\n","epoch 32 iter 22: train loss 1.70116. lr 5.635092e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 33 iter 22: train loss 1.69718. lr 5.612421e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 34 iter 22: train loss 1.66556. lr 5.589115e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 35 iter 22: train loss 1.62890. lr 5.565180e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 36 iter 22: train loss 1.58586. lr 5.540622e-03: 100% 23/23 [00:01<00:00, 14.63it/s]\n","epoch 37 iter 22: train loss 1.62249. lr 5.515446e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 38 iter 22: train loss 1.55430. lr 5.489660e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 39 iter 22: train loss 1.55312. lr 5.463268e-03: 100% 23/23 [00:01<00:00, 14.63it/s]\n","epoch 40 iter 22: train loss 1.56337. lr 5.436278e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 41 iter 22: train loss 1.53492. lr 5.408696e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 42 iter 22: train loss 1.47560. lr 5.380529e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 43 iter 22: train loss 1.51525. lr 5.351784e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 44 iter 22: train loss 1.46891. lr 5.322467e-03: 100% 23/23 [00:01<00:00, 14.65it/s]\n","epoch 45 iter 22: train loss 1.45547. lr 5.292586e-03: 100% 23/23 [00:01<00:00, 14.62it/s]\n","epoch 46 iter 22: train loss 1.44824. lr 5.262148e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 47 iter 22: train loss 1.39901. lr 5.231160e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 48 iter 22: train loss 1.42478. lr 5.199630e-03: 100% 23/23 [00:01<00:00, 14.67it/s]\n","epoch 49 iter 22: train loss 1.35791. lr 5.167566e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 50 iter 22: train loss 1.39813. lr 5.134975e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 51 iter 22: train loss 1.38227. lr 5.101865e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 52 iter 22: train loss 1.34109. lr 5.068245e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 53 iter 22: train loss 1.33400. lr 5.034122e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 54 iter 22: train loss 1.36211. lr 4.999505e-03: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 55 iter 22: train loss 1.31577. lr 4.964402e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 56 iter 22: train loss 1.31133. lr 4.928822e-03: 100% 23/23 [00:01<00:00, 14.63it/s]\n","epoch 57 iter 22: train loss 1.32784. lr 4.892774e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 58 iter 22: train loss 1.31200. lr 4.856265e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 59 iter 22: train loss 1.24179. lr 4.819305e-03: 100% 23/23 [00:01<00:00, 14.74it/s]\n","epoch 60 iter 22: train loss 1.28966. lr 4.781904e-03: 100% 23/23 [00:01<00:00, 14.67it/s]\n","epoch 61 iter 22: train loss 1.26784. lr 4.744069e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 62 iter 22: train loss 1.28068. lr 4.705811e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 63 iter 22: train loss 1.21387. lr 4.667138e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 64 iter 22: train loss 1.18360. lr 4.628061e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 65 iter 22: train loss 1.20058. lr 4.588587e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 66 iter 22: train loss 1.18552. lr 4.548728e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 67 iter 22: train loss 1.22124. lr 4.508492e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 68 iter 22: train loss 1.17916. lr 4.467890e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 69 iter 22: train loss 1.25187. lr 4.426932e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 70 iter 22: train loss 1.17611. lr 4.385626e-03: 100% 23/23 [00:01<00:00, 14.71it/s]\n","epoch 71 iter 22: train loss 1.21533. lr 4.343984e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 72 iter 22: train loss 1.15484. lr 4.302016e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 73 iter 22: train loss 1.14040. lr 4.259731e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 74 iter 22: train loss 1.19162. lr 4.217140e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 75 iter 22: train loss 1.14793. lr 4.174253e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 76 iter 22: train loss 1.13109. lr 4.131081e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 77 iter 22: train loss 1.16243. lr 4.087634e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 78 iter 22: train loss 1.16080. lr 4.043923e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 79 iter 22: train loss 1.11248. lr 3.999958e-03: 100% 23/23 [00:01<00:00, 14.64it/s]\n","epoch 80 iter 22: train loss 1.10898. lr 3.955751e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 81 iter 22: train loss 1.13632. lr 3.911311e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 82 iter 22: train loss 1.09049. lr 3.866650e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 83 iter 22: train loss 1.10408. lr 3.821778e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 84 iter 22: train loss 1.07046. lr 3.776706e-03: 100% 23/23 [00:01<00:00, 14.62it/s]\n","epoch 85 iter 22: train loss 1.04840. lr 3.731446e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 86 iter 22: train loss 1.08040. lr 3.686008e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 87 iter 22: train loss 1.08307. lr 3.640404e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 88 iter 22: train loss 1.07858. lr 3.594643e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 89 iter 22: train loss 1.03351. lr 3.548739e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 90 iter 22: train loss 1.04151. lr 3.502701e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 91 iter 22: train loss 1.06356. lr 3.456541e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 92 iter 22: train loss 1.09190. lr 3.410270e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 93 iter 22: train loss 1.03948. lr 3.363899e-03: 100% 23/23 [00:01<00:00, 14.67it/s]\n","epoch 94 iter 22: train loss 1.01863. lr 3.317440e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 95 iter 22: train loss 1.07658. lr 3.270903e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 96 iter 22: train loss 1.00145. lr 3.224301e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 97 iter 22: train loss 1.03401. lr 3.177645e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 98 iter 22: train loss 1.02307. lr 3.130945e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 99 iter 22: train loss 1.00250. lr 3.084213e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 100 iter 22: train loss 0.98528. lr 3.037461e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 101 iter 22: train loss 1.01235. lr 2.990700e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 102 iter 22: train loss 0.96789. lr 2.943941e-03: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 103 iter 22: train loss 0.98378. lr 2.897196e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 104 iter 22: train loss 1.00155. lr 2.850476e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 105 iter 22: train loss 0.98660. lr 2.803792e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 106 iter 22: train loss 0.98844. lr 2.757156e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 107 iter 22: train loss 0.99494. lr 2.710579e-03: 100% 23/23 [00:01<00:00, 14.65it/s]\n","epoch 108 iter 22: train loss 0.95689. lr 2.664072e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 109 iter 22: train loss 0.96410. lr 2.617646e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 110 iter 22: train loss 0.96797. lr 2.571314e-03: 100% 23/23 [00:01<00:00, 14.66it/s]\n","epoch 111 iter 22: train loss 0.94856. lr 2.525086e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 112 iter 22: train loss 0.96571. lr 2.478973e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 113 iter 22: train loss 0.91565. lr 2.432986e-03: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 114 iter 22: train loss 0.96103. lr 2.387138e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 115 iter 22: train loss 0.94889. lr 2.341438e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 116 iter 22: train loss 0.95023. lr 2.295899e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 117 iter 22: train loss 0.92127. lr 2.250530e-03: 100% 23/23 [00:01<00:00, 14.63it/s]\n","epoch 118 iter 22: train loss 0.89561. lr 2.205344e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 119 iter 22: train loss 0.91395. lr 2.160350e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 120 iter 22: train loss 0.89169. lr 2.115561e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 121 iter 22: train loss 0.90304. lr 2.070986e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 122 iter 22: train loss 0.90017. lr 2.026638e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 123 iter 22: train loss 0.92223. lr 1.982525e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 124 iter 22: train loss 0.90358. lr 1.938660e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 125 iter 22: train loss 0.90473. lr 1.895053e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 126 iter 22: train loss 0.90246. lr 1.851714e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 127 iter 22: train loss 0.90267. lr 1.808654e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 128 iter 22: train loss 0.89083. lr 1.765884e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 129 iter 22: train loss 0.88623. lr 1.723414e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 130 iter 22: train loss 0.88780. lr 1.681253e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 131 iter 22: train loss 0.85833. lr 1.639413e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 132 iter 22: train loss 0.86330. lr 1.597904e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 133 iter 22: train loss 0.88019. lr 1.556735e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 134 iter 22: train loss 0.90976. lr 1.515917e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 135 iter 22: train loss 0.81375. lr 1.475460e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 136 iter 22: train loss 0.84647. lr 1.435373e-03: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 137 iter 22: train loss 0.87637. lr 1.395666e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 138 iter 22: train loss 0.87635. lr 1.356349e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 139 iter 22: train loss 0.86398. lr 1.317431e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 140 iter 22: train loss 0.81979. lr 1.278922e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 141 iter 22: train loss 0.85474. lr 1.240831e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 142 iter 22: train loss 0.81701. lr 1.203168e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 143 iter 22: train loss 0.79913. lr 1.165941e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 144 iter 22: train loss 0.83557. lr 1.129159e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 145 iter 22: train loss 0.83655. lr 1.092833e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 146 iter 22: train loss 0.85126. lr 1.056969e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 147 iter 22: train loss 0.81428. lr 1.021578e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 148 iter 22: train loss 0.84539. lr 9.866675e-04: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 149 iter 22: train loss 0.79699. lr 9.522460e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 150 iter 22: train loss 0.81955. lr 9.183221e-04: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 151 iter 22: train loss 0.80092. lr 8.849040e-04: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 152 iter 22: train loss 0.82040. lr 8.519997e-04: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 153 iter 22: train loss 0.75720. lr 8.196174e-04: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 154 iter 22: train loss 0.80054. lr 7.877647e-04: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 155 iter 22: train loss 0.80645. lr 7.564496e-04: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 156 iter 22: train loss 0.78500. lr 7.256796e-04: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 157 iter 22: train loss 0.79724. lr 6.954621e-04: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 158 iter 22: train loss 0.79429. lr 6.658045e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 159 iter 22: train loss 0.78392. lr 6.367141e-04: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 160 iter 22: train loss 0.78676. lr 6.081979e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 161 iter 22: train loss 0.78809. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 162 iter 22: train loss 0.78708. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 163 iter 22: train loss 0.78311. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 164 iter 22: train loss 0.76706. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 165 iter 22: train loss 0.78373. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 166 iter 22: train loss 0.77176. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 167 iter 22: train loss 0.77363. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 168 iter 22: train loss 0.77337. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 169 iter 22: train loss 0.78955. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 170 iter 22: train loss 0.79001. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 171 iter 22: train loss 0.73991. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 172 iter 22: train loss 0.74972. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.63it/s]\n","epoch 173 iter 22: train loss 0.70652. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 174 iter 22: train loss 0.75536. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 175 iter 22: train loss 0.76350. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 176 iter 22: train loss 0.74920. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.62it/s]\n","epoch 177 iter 22: train loss 0.76558. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 178 iter 22: train loss 0.76762. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 179 iter 22: train loss 0.76946. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 180 iter 22: train loss 0.73840. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 181 iter 22: train loss 0.74465. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 182 iter 22: train loss 0.75429. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 183 iter 22: train loss 0.75787. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 184 iter 22: train loss 0.75930. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 185 iter 22: train loss 0.71430. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 186 iter 22: train loss 0.76870. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 187 iter 22: train loss 0.76865. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 188 iter 22: train loss 0.76194. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 189 iter 22: train loss 0.76081. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 190 iter 22: train loss 0.71338. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 191 iter 22: train loss 0.71474. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 192 iter 22: train loss 0.76341. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 193 iter 22: train loss 0.76339. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 194 iter 22: train loss 0.75401. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 195 iter 22: train loss 0.75801. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 196 iter 22: train loss 0.77056. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 197 iter 22: train loss 0.76085. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 198 iter 22: train loss 0.74452. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 199 iter 22: train loss 0.77124. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 200 iter 22: train loss 0.72462. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 201 iter 22: train loss 0.73779. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 202 iter 22: train loss 0.73812. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 203 iter 22: train loss 0.76430. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 204 iter 22: train loss 0.74096. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 205 iter 22: train loss 0.72071. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 206 iter 22: train loss 0.74806. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 207 iter 22: train loss 0.75622. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 208 iter 22: train loss 0.75567. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 209 iter 22: train loss 0.74433. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 210 iter 22: train loss 0.71847. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 211 iter 22: train loss 0.75101. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 212 iter 22: train loss 0.72777. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 213 iter 22: train loss 0.73828. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 214 iter 22: train loss 0.69663. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 215 iter 22: train loss 0.74549. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 216 iter 22: train loss 0.72515. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 217 iter 22: train loss 0.74335. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 218 iter 22: train loss 0.72810. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 219 iter 22: train loss 0.76849. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 220 iter 22: train loss 0.76797. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 221 iter 22: train loss 0.73981. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 222 iter 22: train loss 0.70686. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.62it/s]\n","epoch 223 iter 22: train loss 0.73820. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 224 iter 22: train loss 0.69373. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 225 iter 22: train loss 0.70359. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 226 iter 22: train loss 0.72448. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 227 iter 22: train loss 0.75191. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 228 iter 22: train loss 0.70368. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 229 iter 22: train loss 0.70740. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 230 iter 22: train loss 0.70878. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 231 iter 22: train loss 0.73223. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 232 iter 22: train loss 0.70509. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 233 iter 22: train loss 0.71569. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 234 iter 22: train loss 0.72192. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 235 iter 22: train loss 0.69322. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 236 iter 22: train loss 0.70560. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 237 iter 22: train loss 0.72455. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 238 iter 22: train loss 0.73147. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 239 iter 22: train loss 0.72602. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 240 iter 22: train loss 0.70310. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 241 iter 22: train loss 0.68696. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 242 iter 22: train loss 0.68381. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 243 iter 22: train loss 0.69569. lr 6.039815e-04: 100% 23/23 [00:01<00:00, 14.64it/s]\n","epoch 244 iter 22: train loss 0.71011. lr 6.324112e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 245 iter 22: train loss 0.70539. lr 6.614162e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 246 iter 22: train loss 0.69628. lr 6.909893e-04: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 247 iter 22: train loss 0.69448. lr 7.211235e-04: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 248 iter 22: train loss 0.70473. lr 7.518114e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 249 iter 22: train loss 0.70810. lr 7.830454e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 250 iter 22: train loss 0.73034. lr 8.148181e-04: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 251 iter 22: train loss 0.69968. lr 8.471217e-04: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 252 iter 22: train loss 0.72589. lr 8.799484e-04: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 253 iter 22: train loss 0.72562. lr 9.132902e-04: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 254 iter 22: train loss 0.73521. lr 9.471389e-04: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 255 iter 22: train loss 0.75556. lr 9.814865e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 256 iter 22: train loss 0.73601. lr 1.016324e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 257 iter 22: train loss 0.73722. lr 1.051644e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 258 iter 22: train loss 0.74563. lr 1.087438e-03: 100% 23/23 [00:01<00:00, 14.27it/s]\n","epoch 259 iter 22: train loss 0.70346. lr 1.123696e-03: 100% 23/23 [00:01<00:00, 13.97it/s]\n","epoch 260 iter 22: train loss 0.71487. lr 1.160409e-03: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 261 iter 22: train loss 0.71185. lr 1.197570e-03: 100% 23/23 [00:01<00:00, 14.00it/s]\n","epoch 262 iter 22: train loss 0.73858. lr 1.235169e-03: 100% 23/23 [00:01<00:00, 13.81it/s]\n","epoch 263 iter 22: train loss 0.76496. lr 1.273196e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 264 iter 22: train loss 0.72699. lr 1.311643e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 265 iter 22: train loss 0.73493. lr 1.350501e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 266 iter 22: train loss 0.75306. lr 1.389759e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 267 iter 22: train loss 0.77936. lr 1.429408e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 268 iter 22: train loss 0.72565. lr 1.469439e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 269 iter 22: train loss 0.78832. lr 1.509841e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 270 iter 22: train loss 0.75247. lr 1.550606e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 271 iter 22: train loss 0.76887. lr 1.591723e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 272 iter 22: train loss 0.78170. lr 1.633182e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 273 iter 22: train loss 0.79093. lr 1.674973e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 274 iter 22: train loss 0.80308. lr 1.717086e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 275 iter 22: train loss 0.74256. lr 1.759511e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 276 iter 22: train loss 0.74600. lr 1.802237e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 277 iter 22: train loss 0.81148. lr 1.845254e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 278 iter 22: train loss 0.78928. lr 1.888552e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 279 iter 22: train loss 0.78780. lr 1.932120e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 280 iter 22: train loss 0.76714. lr 1.975947e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 281 iter 22: train loss 0.74655. lr 2.020023e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 282 iter 22: train loss 0.78277. lr 2.064337e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 283 iter 22: train loss 0.80562. lr 2.108878e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 284 iter 22: train loss 0.75583. lr 2.153636e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 285 iter 22: train loss 0.77113. lr 2.198600e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 286 iter 22: train loss 0.79812. lr 2.243758e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 287 iter 22: train loss 0.79974. lr 2.289100e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 288 iter 22: train loss 0.78513. lr 2.334615e-03: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 289 iter 22: train loss 0.77193. lr 2.380291e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 290 iter 22: train loss 0.78006. lr 2.426118e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 291 iter 22: train loss 0.79678. lr 2.472085e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 292 iter 22: train loss 0.84219. lr 2.518179e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 293 iter 22: train loss 0.81294. lr 2.564391e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 294 iter 22: train loss 0.83714. lr 2.610708e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 295 iter 22: train loss 0.83814. lr 2.657121e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 296 iter 22: train loss 0.78907. lr 2.703616e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 297 iter 22: train loss 0.84093. lr 2.750184e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 298 iter 22: train loss 0.83460. lr 2.796812e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 299 iter 22: train loss 0.81377. lr 2.843489e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 300 iter 22: train loss 0.83958. lr 2.890205e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 301 iter 22: train loss 0.85091. lr 2.936947e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 302 iter 22: train loss 0.85744. lr 2.983705e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 303 iter 22: train loss 0.86548. lr 3.030466e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 304 iter 22: train loss 0.79471. lr 3.077220e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 305 iter 22: train loss 0.85420. lr 3.123955e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 306 iter 22: train loss 0.80154. lr 3.170661e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 307 iter 22: train loss 0.86862. lr 3.217324e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 308 iter 22: train loss 0.85121. lr 3.263935e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 309 iter 22: train loss 0.87189. lr 3.310482e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 310 iter 22: train loss 0.86812. lr 3.356954e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 311 iter 22: train loss 0.83588. lr 3.403338e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 312 iter 22: train loss 0.88213. lr 3.449625e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 313 iter 22: train loss 0.89211. lr 3.495802e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 314 iter 22: train loss 0.82001. lr 3.541859e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 315 iter 22: train loss 0.87806. lr 3.587785e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 316 iter 22: train loss 0.86001. lr 3.633567e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 317 iter 22: train loss 0.86854. lr 3.679196e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 318 iter 22: train loss 0.85499. lr 3.724659e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 319 iter 22: train loss 0.86274. lr 3.769947e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 320 iter 22: train loss 0.89722. lr 3.815047e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 321 iter 22: train loss 0.84575. lr 3.859950e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 322 iter 22: train loss 0.87123. lr 3.904643e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 323 iter 22: train loss 0.90473. lr 3.949117e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 324 iter 22: train loss 0.87039. lr 3.993360e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 325 iter 22: train loss 0.88797. lr 4.037362e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 326 iter 22: train loss 0.90131. lr 4.081111e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 327 iter 22: train loss 0.87325. lr 4.124598e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 328 iter 22: train loss 0.92093. lr 4.167812e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 329 iter 22: train loss 0.87935. lr 4.210742e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 330 iter 22: train loss 0.85585. lr 4.253378e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 331 iter 22: train loss 0.91426. lr 4.295709e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 332 iter 22: train loss 0.90667. lr 4.337726e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 333 iter 22: train loss 0.89108. lr 4.379418e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 334 iter 22: train loss 0.91701. lr 4.420774e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 335 iter 22: train loss 0.94479. lr 4.461785e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 336 iter 22: train loss 0.93001. lr 4.502441e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 337 iter 22: train loss 0.88566. lr 4.542732e-03: 100% 23/23 [00:01<00:00, 14.22it/s]\n","epoch 338 iter 22: train loss 0.91362. lr 4.582648e-03: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 339 iter 22: train loss 0.87762. lr 4.622180e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 340 iter 22: train loss 0.85681. lr 4.661318e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 341 iter 22: train loss 0.93427. lr 4.700052e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 342 iter 22: train loss 0.93747. lr 4.738372e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 343 iter 22: train loss 0.89477. lr 4.776271e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 344 iter 22: train loss 0.87288. lr 4.813738e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 345 iter 22: train loss 0.89787. lr 4.850764e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 346 iter 22: train loss 0.89055. lr 4.887341e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 347 iter 22: train loss 0.94410. lr 4.923459e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 348 iter 22: train loss 0.90864. lr 4.959110e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 349 iter 22: train loss 0.91847. lr 4.994284e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 350 iter 22: train loss 0.91533. lr 5.028975e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 351 iter 22: train loss 0.89117. lr 5.063172e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 352 iter 22: train loss 0.89387. lr 5.096868e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 353 iter 22: train loss 0.91459. lr 5.130054e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 354 iter 22: train loss 0.94249. lr 5.162723e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 355 iter 22: train loss 0.93069. lr 5.194867e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 356 iter 22: train loss 0.93732. lr 5.226477e-03: 100% 23/23 [00:01<00:00, 14.19it/s]\n","epoch 357 iter 22: train loss 0.93382. lr 5.257546e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 358 iter 22: train loss 0.93226. lr 5.288067e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 359 iter 22: train loss 0.90262. lr 5.318032e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 360 iter 22: train loss 0.89314. lr 5.347434e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 361 iter 22: train loss 0.91607. lr 5.376265e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 362 iter 22: train loss 0.88694. lr 5.404519e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 363 iter 22: train loss 0.91975. lr 5.432189e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 364 iter 22: train loss 0.91515. lr 5.459268e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 365 iter 22: train loss 0.95817. lr 5.485750e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 366 iter 22: train loss 0.92309. lr 5.511627e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 367 iter 22: train loss 0.91608. lr 5.536894e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 368 iter 22: train loss 0.92402. lr 5.561545e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 369 iter 22: train loss 0.91993. lr 5.585574e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 370 iter 22: train loss 0.86256. lr 5.608974e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 371 iter 22: train loss 0.90387. lr 5.631741e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 372 iter 22: train loss 0.91794. lr 5.653868e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 373 iter 22: train loss 0.90782. lr 5.675350e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 374 iter 22: train loss 0.90934. lr 5.696182e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 375 iter 22: train loss 0.88969. lr 5.716359e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 376 iter 22: train loss 0.86933. lr 5.735876e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 377 iter 22: train loss 0.90199. lr 5.754729e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 378 iter 22: train loss 0.88603. lr 5.772912e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 379 iter 22: train loss 0.87281. lr 5.790422e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 380 iter 22: train loss 0.88436. lr 5.807253e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 381 iter 22: train loss 0.89904. lr 5.823403e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 382 iter 22: train loss 0.87850. lr 5.838866e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 383 iter 22: train loss 0.87686. lr 5.853640e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 384 iter 22: train loss 0.86841. lr 5.867720e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 385 iter 22: train loss 0.90024. lr 5.881104e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 386 iter 22: train loss 0.92627. lr 5.893788e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 387 iter 22: train loss 0.90948. lr 5.905768e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 388 iter 22: train loss 0.85859. lr 5.917043e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 389 iter 22: train loss 0.91069. lr 5.927609e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 390 iter 22: train loss 0.89374. lr 5.937464e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 391 iter 22: train loss 0.89125. lr 5.946605e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 392 iter 22: train loss 0.87347. lr 5.955030e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 393 iter 22: train loss 0.87153. lr 5.962737e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 394 iter 22: train loss 0.86755. lr 5.969724e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 395 iter 22: train loss 0.84811. lr 5.975990e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 396 iter 22: train loss 0.89348. lr 5.981532e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 397 iter 22: train loss 0.90210. lr 5.986351e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 398 iter 22: train loss 0.84019. lr 5.990443e-03: 100% 23/23 [00:01<00:00, 14.24it/s]\n","epoch 399 iter 22: train loss 0.90013. lr 5.993809e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 400 iter 22: train loss 0.88247. lr 5.996448e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 401 iter 22: train loss 0.83466. lr 5.998359e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 402 iter 22: train loss 0.87543. lr 5.999541e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 403 iter 22: train loss 0.87265. lr 5.999995e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 404 iter 22: train loss 0.84135. lr 5.999719e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 405 iter 22: train loss 0.83961. lr 5.998715e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 406 iter 22: train loss 0.86723. lr 5.996982e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 407 iter 22: train loss 0.86075. lr 5.994521e-03: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 408 iter 22: train loss 0.85179. lr 5.991333e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 409 iter 22: train loss 0.85499. lr 5.987417e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 410 iter 22: train loss 0.86545. lr 5.982776e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 411 iter 22: train loss 0.84838. lr 5.977411e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 412 iter 22: train loss 0.81062. lr 5.971321e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 413 iter 22: train loss 0.83877. lr 5.964510e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 414 iter 22: train loss 0.86328. lr 5.956979e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 415 iter 22: train loss 0.86784. lr 5.948729e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 416 iter 22: train loss 0.83941. lr 5.939763e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 417 iter 22: train loss 0.87369. lr 5.930082e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 418 iter 22: train loss 0.87256. lr 5.919690e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 419 iter 22: train loss 0.82485. lr 5.908588e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 420 iter 22: train loss 0.83389. lr 5.896780e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 421 iter 22: train loss 0.82624. lr 5.884268e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 422 iter 22: train loss 0.84027. lr 5.871055e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 423 iter 22: train loss 0.82106. lr 5.857144e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 424 iter 22: train loss 0.80326. lr 5.842539e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 425 iter 22: train loss 0.86843. lr 5.827244e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 426 iter 22: train loss 0.85706. lr 5.811262e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 427 iter 22: train loss 0.84013. lr 5.794596e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 428 iter 22: train loss 0.86261. lr 5.777252e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 429 iter 22: train loss 0.82507. lr 5.759233e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 430 iter 22: train loss 0.82467. lr 5.740544e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 431 iter 22: train loss 0.84772. lr 5.721189e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 432 iter 22: train loss 0.82378. lr 5.701172e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 433 iter 22: train loss 0.83417. lr 5.680499e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 434 iter 22: train loss 0.82714. lr 5.659176e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 435 iter 22: train loss 0.81967. lr 5.637206e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 436 iter 22: train loss 0.84579. lr 5.614595e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 437 iter 22: train loss 0.80428. lr 5.591349e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 438 iter 22: train loss 0.85576. lr 5.567473e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 439 iter 22: train loss 0.83476. lr 5.542974e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 440 iter 22: train loss 0.80972. lr 5.517857e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 441 iter 22: train loss 0.81319. lr 5.492128e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 442 iter 22: train loss 0.85683. lr 5.465793e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 443 iter 22: train loss 0.79906. lr 5.438860e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 444 iter 22: train loss 0.83546. lr 5.411333e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 445 iter 22: train loss 0.78197. lr 5.383222e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 446 iter 22: train loss 0.79050. lr 5.354531e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 447 iter 22: train loss 0.84210. lr 5.325267e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 448 iter 22: train loss 0.81172. lr 5.295439e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 449 iter 22: train loss 0.79642. lr 5.265054e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 450 iter 22: train loss 0.78354. lr 5.234118e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 451 iter 22: train loss 0.80906. lr 5.202639e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 452 iter 22: train loss 0.77950. lr 5.170625e-03: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 453 iter 22: train loss 0.80641. lr 5.138084e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 454 iter 22: train loss 0.75176. lr 5.105023e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 455 iter 22: train loss 0.79139. lr 5.071450e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 456 iter 22: train loss 0.79119. lr 5.037375e-03: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 457 iter 22: train loss 0.82044. lr 5.002804e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 458 iter 22: train loss 0.75369. lr 4.967747e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 459 iter 22: train loss 0.79822. lr 4.932212e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 460 iter 22: train loss 0.77464. lr 4.896207e-03: 100% 23/23 [00:01<00:00, 14.26it/s]\n","epoch 461 iter 22: train loss 0.74457. lr 4.859742e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 462 iter 22: train loss 0.76722. lr 4.822825e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 463 iter 22: train loss 0.76407. lr 4.785465e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 464 iter 22: train loss 0.79989. lr 4.747671e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 465 iter 22: train loss 0.80819. lr 4.709452e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 466 iter 22: train loss 0.74541. lr 4.670818e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 467 iter 22: train loss 0.73402. lr 4.631778e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 468 iter 22: train loss 0.75489. lr 4.592342e-03: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 469 iter 22: train loss 0.73124. lr 4.552519e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 470 iter 22: train loss 0.73810. lr 4.512319e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 471 iter 22: train loss 0.76947. lr 4.471751e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 472 iter 22: train loss 0.73280. lr 4.430825e-03: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 473 iter 22: train loss 0.75330. lr 4.389552e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 474 iter 22: train loss 0.76991. lr 4.347942e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 475 iter 22: train loss 0.77173. lr 4.306004e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 476 iter 22: train loss 0.78164. lr 4.263748e-03: 100% 23/23 [00:01<00:00, 14.29it/s]\n","epoch 477 iter 22: train loss 0.77358. lr 4.221186e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 478 iter 22: train loss 0.73377. lr 4.178327e-03: 100% 23/23 [00:01<00:00, 14.33it/s]\n","epoch 479 iter 22: train loss 0.69623. lr 4.135181e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 480 iter 22: train loss 0.74417. lr 4.091760e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 481 iter 22: train loss 0.74527. lr 4.048074e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 482 iter 22: train loss 0.70288. lr 4.004132e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 483 iter 22: train loss 0.73847. lr 3.959947e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 484 iter 22: train loss 0.70893. lr 3.915529e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 485 iter 22: train loss 0.71268. lr 3.870888e-03: 100% 23/23 [00:01<00:00, 14.63it/s]\n","epoch 486 iter 22: train loss 0.73327. lr 3.826036e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 487 iter 22: train loss 0.68671. lr 3.780983e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 488 iter 22: train loss 0.71824. lr 3.735740e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 489 iter 22: train loss 0.69987. lr 3.690318e-03: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 490 iter 22: train loss 0.69285. lr 3.644729e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 491 iter 22: train loss 0.74271. lr 3.598983e-03: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 492 iter 22: train loss 0.68836. lr 3.553092e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 493 iter 22: train loss 0.68787. lr 3.507066e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 494 iter 22: train loss 0.71043. lr 3.460917e-03: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 495 iter 22: train loss 0.74056. lr 3.414656e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 496 iter 22: train loss 0.68793. lr 3.368294e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 497 iter 22: train loss 0.69954. lr 3.321843e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 498 iter 22: train loss 0.70658. lr 3.275313e-03: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 499 iter 22: train loss 0.71811. lr 3.228717e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 500 iter 22: train loss 0.71852. lr 3.182065e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 501 iter 22: train loss 0.66304. lr 3.135369e-03: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 502 iter 22: train loss 0.68809. lr 3.088640e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 503 iter 22: train loss 0.65080. lr 3.041889e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 504 iter 22: train loss 0.65012. lr 2.995129e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 505 iter 22: train loss 0.70655. lr 2.948369e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 506 iter 22: train loss 0.70632. lr 2.901622e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 507 iter 22: train loss 0.67149. lr 2.854899e-03: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 508 iter 22: train loss 0.69097. lr 2.808211e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 509 iter 22: train loss 0.63721. lr 2.761570e-03: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 510 iter 22: train loss 0.63735. lr 2.714987e-03: 100% 23/23 [00:01<00:00, 14.57it/s]\n","epoch 511 iter 22: train loss 0.64285. lr 2.668473e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 512 iter 22: train loss 0.67193. lr 2.622039e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 513 iter 22: train loss 0.64278. lr 2.575697e-03: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 514 iter 22: train loss 0.63069. lr 2.529459e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 515 iter 22: train loss 0.61949. lr 2.483334e-03: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 516 iter 22: train loss 0.65397. lr 2.437336e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 517 iter 22: train loss 0.64204. lr 2.391474e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 518 iter 22: train loss 0.64209. lr 2.345759e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 519 iter 22: train loss 0.63181. lr 2.300204e-03: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 520 iter 22: train loss 0.62945. lr 2.254819e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 521 iter 22: train loss 0.63818. lr 2.209615e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 522 iter 22: train loss 0.64599. lr 2.164603e-03: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 523 iter 22: train loss 0.64882. lr 2.119793e-03: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 524 iter 22: train loss 0.62911. lr 2.075198e-03: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 525 iter 22: train loss 0.60697. lr 2.030827e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 526 iter 22: train loss 0.61187. lr 1.986692e-03: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 527 iter 22: train loss 0.64906. lr 1.942803e-03: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 528 iter 22: train loss 0.62801. lr 1.899171e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 529 iter 22: train loss 0.64811. lr 1.855807e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 530 iter 22: train loss 0.59143. lr 1.812720e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 531 iter 22: train loss 0.60017. lr 1.769922e-03: 100% 23/23 [00:01<00:00, 14.20it/s]\n","epoch 532 iter 22: train loss 0.59452. lr 1.727422e-03: 100% 23/23 [00:01<00:00, 14.17it/s]\n","epoch 533 iter 22: train loss 0.61188. lr 1.685232e-03: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 534 iter 22: train loss 0.61153. lr 1.643361e-03: 100% 23/23 [00:01<00:00, 14.18it/s]\n","epoch 535 iter 22: train loss 0.61197. lr 1.601820e-03: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 536 iter 22: train loss 0.63949. lr 1.560619e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 537 iter 22: train loss 0.64156. lr 1.519767e-03: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 538 iter 22: train loss 0.58238. lr 1.479275e-03: 100% 23/23 [00:01<00:00, 14.65it/s]\n","epoch 539 iter 22: train loss 0.59026. lr 1.439153e-03: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 540 iter 22: train loss 0.59553. lr 1.399409e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 541 iter 22: train loss 0.60930. lr 1.360055e-03: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 542 iter 22: train loss 0.58923. lr 1.321099e-03: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 543 iter 22: train loss 0.63596. lr 1.282551e-03: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 544 iter 22: train loss 0.57929. lr 1.244420e-03: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 545 iter 22: train loss 0.62977. lr 1.206716e-03: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 546 iter 22: train loss 0.61321. lr 1.169447e-03: 100% 23/23 [00:01<00:00, 14.23it/s]\n","epoch 547 iter 22: train loss 0.60102. lr 1.132623e-03: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 548 iter 22: train loss 0.57380. lr 1.096253e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 549 iter 22: train loss 0.58044. lr 1.060345e-03: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 550 iter 22: train loss 0.54920. lr 1.024909e-03: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 551 iter 22: train loss 0.54770. lr 9.899527e-04: 100% 23/23 [00:01<00:00, 14.32it/s]\n","epoch 552 iter 22: train loss 0.58220. lr 9.554845e-04: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 553 iter 22: train loss 0.55729. lr 9.215132e-04: 100% 23/23 [00:01<00:00, 14.25it/s]\n","epoch 554 iter 22: train loss 0.54355. lr 8.880468e-04: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 555 iter 22: train loss 0.54422. lr 8.550935e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 556 iter 22: train loss 0.56028. lr 8.226614e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 557 iter 22: train loss 0.57568. lr 7.907583e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 558 iter 22: train loss 0.57517. lr 7.593919e-04: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 559 iter 22: train loss 0.56087. lr 7.285699e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 560 iter 22: train loss 0.54409. lr 6.982998e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 561 iter 22: train loss 0.54746. lr 6.685889e-04: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 562 iter 22: train loss 0.58804. lr 6.394445e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 563 iter 22: train loss 0.52593. lr 6.108735e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 564 iter 22: train loss 0.56430. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 565 iter 22: train loss 0.53577. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 566 iter 22: train loss 0.55770. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 567 iter 22: train loss 0.52186. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 568 iter 22: train loss 0.57732. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 569 iter 22: train loss 0.52307. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 570 iter 22: train loss 0.52538. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 571 iter 22: train loss 0.57492. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 572 iter 22: train loss 0.55823. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 573 iter 22: train loss 0.52483. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 574 iter 22: train loss 0.54593. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 575 iter 22: train loss 0.59317. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 576 iter 22: train loss 0.55437. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 577 iter 22: train loss 0.53935. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 578 iter 22: train loss 0.53398. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.36it/s]\n","epoch 579 iter 22: train loss 0.52598. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 580 iter 22: train loss 0.55424. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 581 iter 22: train loss 0.54595. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 582 iter 22: train loss 0.58846. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 583 iter 22: train loss 0.53847. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 584 iter 22: train loss 0.52039. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 585 iter 22: train loss 0.54628. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 586 iter 22: train loss 0.52957. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 587 iter 22: train loss 0.54048. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 588 iter 22: train loss 0.53990. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.49it/s]\n","epoch 589 iter 22: train loss 0.53443. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.31it/s]\n","epoch 590 iter 22: train loss 0.51764. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 591 iter 22: train loss 0.52184. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 592 iter 22: train loss 0.52190. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 593 iter 22: train loss 0.52227. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 594 iter 22: train loss 0.56497. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 595 iter 22: train loss 0.56650. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.65it/s]\n","epoch 596 iter 22: train loss 0.56102. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 597 iter 22: train loss 0.52433. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 598 iter 22: train loss 0.51738. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 599 iter 22: train loss 0.54898. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 600 iter 22: train loss 0.50153. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.59it/s]\n","epoch 601 iter 22: train loss 0.53193. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 602 iter 22: train loss 0.51601. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.48it/s]\n","epoch 603 iter 22: train loss 0.53311. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 604 iter 22: train loss 0.52414. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 605 iter 22: train loss 0.52526. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 606 iter 22: train loss 0.50612. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.39it/s]\n","epoch 607 iter 22: train loss 0.52479. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.40it/s]\n","epoch 608 iter 22: train loss 0.53045. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 609 iter 22: train loss 0.54651. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.58it/s]\n","epoch 610 iter 22: train loss 0.53024. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.30it/s]\n","epoch 611 iter 22: train loss 0.53164. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 612 iter 22: train loss 0.52264. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.61it/s]\n","epoch 613 iter 22: train loss 0.56555. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 614 iter 22: train loss 0.52254. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.56it/s]\n","epoch 615 iter 22: train loss 0.49998. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 616 iter 22: train loss 0.55857. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 617 iter 22: train loss 0.50600. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.50it/s]\n","epoch 618 iter 22: train loss 0.52731. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 619 iter 22: train loss 0.55762. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.28it/s]\n","epoch 620 iter 22: train loss 0.53219. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 621 iter 22: train loss 0.56611. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 622 iter 22: train loss 0.52763. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 623 iter 22: train loss 0.57453. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.34it/s]\n","epoch 624 iter 22: train loss 0.50393. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.35it/s]\n","epoch 625 iter 22: train loss 0.52803. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 626 iter 22: train loss 0.50233. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.44it/s]\n","epoch 627 iter 22: train loss 0.57954. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 628 iter 22: train loss 0.55432. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.42it/s]\n","epoch 629 iter 22: train loss 0.55702. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.38it/s]\n","epoch 630 iter 22: train loss 0.53699. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 631 iter 22: train loss 0.52677. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.45it/s]\n","epoch 632 iter 22: train loss 0.55367. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 633 iter 22: train loss 0.53915. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 634 iter 22: train loss 0.51101. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 635 iter 22: train loss 0.53237. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 636 iter 22: train loss 0.53631. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 637 iter 22: train loss 0.53945. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 638 iter 22: train loss 0.51869. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.53it/s]\n","epoch 639 iter 22: train loss 0.53139. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.43it/s]\n","epoch 640 iter 22: train loss 0.52898. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.60it/s]\n","epoch 641 iter 22: train loss 0.55412. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.55it/s]\n","epoch 642 iter 22: train loss 0.53598. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 643 iter 22: train loss 0.52069. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.37it/s]\n","epoch 644 iter 22: train loss 0.52984. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.51it/s]\n","epoch 645 iter 22: train loss 0.51045. lr 6.000000e-04: 100% 23/23 [00:01<00:00, 14.52it/s]\n","epoch 646 iter 22: train loss 0.55711. lr 6.013192e-04: 100% 23/23 [00:01<00:00, 14.41it/s]\n","epoch 647 iter 22: train loss 0.54678. lr 6.296941e-04: 100% 23/23 [00:01<00:00, 14.46it/s]\n","epoch 648 iter 22: train loss 0.52870. lr 6.586449e-04: 100% 23/23 [00:01<00:00, 14.47it/s]\n","epoch 649 iter 22: train loss 0.53805. lr 6.881646e-04: 100% 23/23 [00:01<00:00, 14.54it/s]\n","epoch 650 iter 22: train loss 0.51305. lr 7.182460e-04: 100% 23/23 [00:01<00:00, 14.52it/s]\n","data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","epoch 1 iter 7: train loss 0.74677. lr 5.999844e-04: 100% 8/8 [00:01<00:00,  7.88it/s]\n","epoch 2 iter 7: train loss 0.64180. lr 5.999351e-04: 100% 8/8 [00:00<00:00,  8.19it/s]\n","epoch 3 iter 7: train loss 0.57504. lr 5.998520e-04: 100% 8/8 [00:00<00:00,  8.16it/s]\n","epoch 4 iter 7: train loss 0.52177. lr 5.997351e-04: 100% 8/8 [00:00<00:00,  8.23it/s]\n","epoch 5 iter 7: train loss 0.48468. lr 5.995844e-04: 100% 8/8 [00:00<00:00,  8.30it/s]\n","epoch 6 iter 7: train loss 0.42106. lr 5.993999e-04: 100% 8/8 [00:00<00:00,  8.36it/s]\n","epoch 7 iter 7: train loss 0.35581. lr 5.991818e-04: 100% 8/8 [00:00<00:00,  8.25it/s]\n","epoch 8 iter 7: train loss 0.28416. lr 5.989299e-04: 100% 8/8 [00:00<00:00,  8.25it/s]\n","epoch 9 iter 7: train loss 0.24464. lr 5.986444e-04: 100% 8/8 [00:00<00:00,  8.18it/s]\n","epoch 10 iter 7: train loss 0.20291. lr 5.983252e-04: 100% 8/8 [00:00<00:00,  8.05it/s]\n","epoch 11 iter 7: train loss 0.16769. lr 5.979723e-04: 100% 8/8 [00:00<00:00,  8.19it/s]\n","epoch 12 iter 7: train loss 0.12818. lr 5.975860e-04: 100% 8/8 [00:00<00:00,  8.26it/s]\n","epoch 13 iter 7: train loss 0.10555. lr 5.971660e-04: 100% 8/8 [00:00<00:00,  8.31it/s]\n","epoch 14 iter 7: train loss 0.10107. lr 5.967127e-04: 100% 8/8 [00:00<00:00,  8.28it/s]\n","epoch 15 iter 7: train loss 0.06984. lr 5.962258e-04: 100% 8/8 [00:00<00:00,  8.19it/s]\n","epoch 16 iter 7: train loss 0.06304. lr 5.957056e-04: 100% 8/8 [00:00<00:00,  8.29it/s]\n","epoch 17 iter 7: train loss 0.05947. lr 5.951521e-04: 100% 8/8 [00:00<00:00,  8.14it/s]\n","epoch 18 iter 7: train loss 0.05259. lr 5.945654e-04: 100% 8/8 [00:00<00:00,  8.25it/s]\n","epoch 19 iter 7: train loss 0.04602. lr 5.939454e-04: 100% 8/8 [00:00<00:00,  8.19it/s]\n","epoch 20 iter 7: train loss 0.03892. lr 5.932923e-04: 100% 8/8 [00:00<00:00,  8.15it/s]\n","epoch 21 iter 7: train loss 0.03152. lr 5.926062e-04: 100% 8/8 [00:01<00:00,  7.91it/s]\n","epoch 22 iter 7: train loss 0.03194. lr 5.918871e-04: 100% 8/8 [00:00<00:00,  8.15it/s]\n","epoch 23 iter 7: train loss 0.02797. lr 5.911352e-04: 100% 8/8 [00:00<00:00,  8.15it/s]\n","epoch 24 iter 7: train loss 0.03905. lr 5.903504e-04: 100% 8/8 [00:00<00:00,  8.12it/s]\n","epoch 25 iter 7: train loss 0.02231. lr 5.895329e-04: 100% 8/8 [00:00<00:00,  8.20it/s]\n","epoch 26 iter 7: train loss 0.02477. lr 5.886828e-04: 100% 8/8 [00:00<00:00,  8.17it/s]\n","epoch 27 iter 7: train loss 0.02018. lr 5.878002e-04: 100% 8/8 [00:00<00:00,  8.20it/s]\n","epoch 28 iter 7: train loss 0.02480. lr 5.868851e-04: 100% 8/8 [00:00<00:00,  8.18it/s]\n","epoch 29 iter 7: train loss 0.02427. lr 5.859378e-04: 100% 8/8 [00:00<00:00,  8.28it/s]\n","epoch 30 iter 7: train loss 0.02147. lr 5.849582e-04: 100% 8/8 [00:00<00:00,  8.30it/s]\n","epoch 31 iter 7: train loss 0.01384. lr 5.839465e-04: 100% 8/8 [00:00<00:00,  8.19it/s]\n","epoch 32 iter 7: train loss 0.01864. lr 5.829028e-04: 100% 8/8 [00:00<00:00,  8.16it/s]\n","epoch 33 iter 7: train loss 0.01906. lr 5.818272e-04: 100% 8/8 [00:00<00:00,  8.26it/s]\n","epoch 34 iter 7: train loss 0.01511. lr 5.807199e-04: 100% 8/8 [00:00<00:00,  8.14it/s]\n","epoch 35 iter 7: train loss 0.01503. lr 5.795810e-04: 100% 8/8 [00:00<00:00,  8.23it/s]\n","epoch 36 iter 7: train loss 0.01597. lr 5.784106e-04: 100% 8/8 [00:00<00:00,  8.32it/s]\n","epoch 37 iter 7: train loss 0.01413. lr 5.772087e-04: 100% 8/8 [00:00<00:00,  8.27it/s]\n","epoch 38 iter 7: train loss 0.01584. lr 5.759757e-04: 100% 8/8 [00:00<00:00,  8.27it/s]\n","epoch 39 iter 7: train loss 0.01289. lr 5.747116e-04: 100% 8/8 [00:00<00:00,  8.24it/s]\n","epoch 40 iter 7: train loss 0.01330. lr 5.734165e-04: 100% 8/8 [00:00<00:00,  8.17it/s]\n","epoch 41 iter 7: train loss 0.01287. lr 5.720906e-04: 100% 8/8 [00:00<00:00,  8.17it/s]\n","epoch 42 iter 7: train loss 0.01267. lr 5.707341e-04: 100% 8/8 [00:00<00:00,  8.27it/s]\n","epoch 43 iter 7: train loss 0.01228. lr 5.693470e-04: 100% 8/8 [00:00<00:00,  8.25it/s]\n","epoch 44 iter 7: train loss 0.01263. lr 5.679296e-04: 100% 8/8 [00:00<00:00,  8.27it/s]\n","epoch 45 iter 7: train loss 0.02140. lr 5.664820e-04: 100% 8/8 [00:00<00:00,  8.21it/s]\n","epoch 46 iter 7: train loss 0.01252. lr 5.650044e-04: 100% 8/8 [00:00<00:00,  8.25it/s]\n","epoch 47 iter 7: train loss 0.01556. lr 5.634970e-04: 100% 8/8 [00:00<00:00,  8.26it/s]\n","epoch 48 iter 7: train loss 0.00893. lr 5.619598e-04: 100% 8/8 [00:00<00:00,  8.22it/s]\n","epoch 49 iter 7: train loss 0.00822. lr 5.603932e-04: 100% 8/8 [00:00<00:00,  8.16it/s]\n","epoch 50 iter 7: train loss 0.00657. lr 5.587972e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 51 iter 7: train loss 0.01326. lr 5.571720e-04: 100% 8/8 [00:00<00:00,  8.24it/s]\n","epoch 52 iter 7: train loss 0.01224. lr 5.555179e-04: 100% 8/8 [00:00<00:00,  8.21it/s]\n","epoch 53 iter 7: train loss 0.00861. lr 5.538350e-04: 100% 8/8 [00:00<00:00,  8.11it/s]\n","epoch 54 iter 7: train loss 0.00681. lr 5.521235e-04: 100% 8/8 [00:00<00:00,  8.20it/s]\n","epoch 55 iter 7: train loss 0.01269. lr 5.503835e-04: 100% 8/8 [00:00<00:00,  8.26it/s]\n","epoch 56 iter 7: train loss 0.00677. lr 5.486154e-04: 100% 8/8 [00:00<00:00,  8.24it/s]\n","epoch 57 iter 7: train loss 0.00590. lr 5.468193e-04: 100% 8/8 [00:00<00:00,  8.11it/s]\n","epoch 58 iter 7: train loss 0.00627. lr 5.449953e-04: 100% 8/8 [00:00<00:00,  8.17it/s]\n","epoch 59 iter 7: train loss 0.01222. lr 5.431438e-04: 100% 8/8 [00:00<00:00,  8.33it/s]\n","epoch 60 iter 7: train loss 0.01071. lr 5.412648e-04: 100% 8/8 [00:00<00:00,  8.16it/s]\n","epoch 61 iter 7: train loss 0.00532. lr 5.393587e-04: 100% 8/8 [00:00<00:00,  8.19it/s]\n","epoch 62 iter 7: train loss 0.00828. lr 5.374256e-04: 100% 8/8 [00:00<00:00,  8.24it/s]\n","epoch 63 iter 7: train loss 0.00841. lr 5.354657e-04: 100% 8/8 [00:00<00:00,  8.22it/s]\n","epoch 64 iter 7: train loss 0.00675. lr 5.334794e-04: 100% 8/8 [00:00<00:00,  8.25it/s]\n","epoch 65 iter 7: train loss 0.00837. lr 5.314667e-04: 100% 8/8 [00:00<00:00,  8.30it/s]\n","epoch 66 iter 7: train loss 0.00866. lr 5.294279e-04: 100% 8/8 [00:00<00:00,  8.26it/s]\n","epoch 67 iter 7: train loss 0.00848. lr 5.273633e-04: 100% 8/8 [00:00<00:00,  8.32it/s]\n","epoch 68 iter 7: train loss 0.00948. lr 5.252731e-04: 100% 8/8 [00:00<00:00,  8.22it/s]\n","epoch 69 iter 7: train loss 0.00510. lr 5.231575e-04: 100% 8/8 [00:00<00:00,  8.10it/s]\n","epoch 70 iter 7: train loss 0.00791. lr 5.210167e-04: 100% 8/8 [00:00<00:00,  8.18it/s]\n","epoch 71 iter 7: train loss 0.00646. lr 5.188511e-04: 100% 8/8 [00:00<00:00,  8.37it/s]\n","epoch 72 iter 7: train loss 0.00891. lr 5.166608e-04: 100% 8/8 [00:00<00:00,  8.21it/s]\n","epoch 73 iter 7: train loss 0.00698. lr 5.144461e-04: 100% 8/8 [00:00<00:00,  8.08it/s]\n","epoch 74 iter 7: train loss 0.00781. lr 5.122072e-04: 100% 8/8 [00:00<00:00,  8.18it/s]\n","epoch 75 iter 7: train loss 0.00705. lr 5.099444e-04: 100% 8/8 [00:00<00:00,  8.19it/s]\n","data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","500it [00:41, 12.09it/s]\n","Correct: 73.0 out of 500.0: 14.6%\n","data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","437it [00:36, 12.09it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to synthesizer.pretrain.test.predictions; no targets provided\n"],"name":"stdout"}]}]}